{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, isnan, isnull, when, lit, count, desc, first,  regexp_extract, lag,to_date, avg, datediff, sum\n",
    "from pyspark.sql.types import NumericType, StringType, FloatType, StructType, StructField\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "spark = SparkSession.builder.appName(\"MS3\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df = spark.read.parquet(\"./fintech_data_43_52_0812.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         Customer Id|           Emp Title|Emp Length|Home Ownership|Annual Inc|Annual Inc Joint|Verification Status|Zip Code|Addr State|Avg Cur Bal|Tot Cur Bal|Loan Id|       Loan Status|Loan Amount|State|Funded Amount|      Term|Int Rate|Grade|       Issue Date|Pymnt Plan|      Type|           Purpose|         Description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidPVlx4ZTdceDkwN...|Medical Policy Nu...|   2 years|           OWN|   54000.0|            NULL|           Verified|   210xx|        MD|     5475.0|    54749.0|  69677|       Charged Off|     8000.0|   MD|       8000.0| 36 months|  0.2274|   21|    17 April 2017|     false|Individual|           medical|    Medical expenses|\n",
      "|YidceDAyXHgwMFx4Z...|              trller| 10+ years|          RENT|   37500.0|            NULL|    Source Verified|   988xx|        WA|     3478.0|    38257.0|  85890|           Current|    10000.0|   WA|      10000.0| 36 months|  0.0769|    3|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidBXHg5Ylx4ZmExe...|           Clinician|   6 years|      MORTGAGE|   72000.0|            NULL|    Source Verified|   088xx|        NJ|     2038.0|    34640.0| 149260|        Fully Paid|    15000.0|   NJ|      15000.0| 36 months|  0.0993|    9|  18 January 2018|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDBmXHg4YVx4Z...|Assistant US Atto...| 10+ years|          RENT|  158600.0|            NULL|    Source Verified|   021xx|        MA|     6129.0|    42904.0| 254638|           Current|    35000.0|   MA|      35000.0| 36 months|  0.0916|    7| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YiJceDg2SlIwXHg3Z...|Account Represent...|   5 years|          RENT|   35000.0|            NULL|       Not Verified|   330xx|        FL|     1492.0|    28345.0|  80248|        Fully Paid|     9600.0|   FL|       9600.0| 36 months|  0.1498|   11| 13 December 2013|     false|INDIVIDUAL|debt_consolidation|  Debt Consolidation|\n",
      "|YidceGQ1Mlx4MDhce...|Lead Client Servi...|   8 years|          RENT|   35000.0|            NULL|    Source Verified|   011xx|        MA|     8524.0|    68190.0|  21975|        Fully Paid|     4500.0|   MA|       4500.0| 36 months|  0.0789|    3|  15 October 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGM4XHgwMlx4Y...|             Produce|   2 years|          RENT|   15000.0|         65000.0|    Source Verified|   723xx|        AR|     1011.0|     9100.0|  18997|           Current|     4000.0|   AR|       4000.0| 36 months|  0.1358|   12|    18 April 2018|     false| Joint App|debt_consolidation|  Debt consolidation|\n",
      "|YicuXHhlNFx4MGNce...|           Mechanic | 10+ years|          RENT|   80000.0|            NULL|    Source Verified|   107xx|        NY|     6639.0|    66387.0| 234661|           Current|    28000.0|   NY|      28000.0| 36 months|  0.0944|   10|  17 October 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFkXHhkMVx4Z...|                  RN| 10+ years|          RENT|   90000.0|            NULL|       Not Verified|   023xx|        MA|     4308.0|    56009.0|  91392|Late (31-120 days)|    10000.0|   MA|      10000.0| 36 months|  0.1144|   10|    17 March 2017|     false|Individual|             other|               Other|\n",
      "|YidceGQ1WVx4YmVce...|Assistant Store M...|   6 years|          RENT|   65000.0|            NULL|    Source Verified|   109xx|        NY|     2237.0|    20135.0|  66884|           Current|     8000.0|   NY|       8000.0| 36 months|    NULL|   14| 16 November 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGUxXHhlNFx4Y...|Director of Facil...|   7 years|      MORTGAGE|  165000.0|            NULL|           Verified|   857xx|        AZ|    18621.0|   316554.0| 251397|           Current|    33100.0|   AZ|      33100.0| 36 months|  0.1367|   14|     16 June 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFjXHg4YVx4M...|    Registered Nurse|   2 years|           OWN|   60000.0|            NULL|       Not Verified|   900xx|        CA|     3440.0|    51604.0| 124434|           Current|    12000.0|   CA|      12000.0| 60 months|  0.0899|    9|     16 July 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidqIUcmYihceDAyX...|        Truck driver|   3 years|          RENT|   80000.0|            NULL|       Not Verified|   752xx|        TX|     3569.0|    32122.0|  31146|           Current|     5000.0|   TX|       5000.0| 36 months|  0.1308|    9|   19 August 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|Yic3XHhmNlx4Yzlce...|Guest Services As...|   2 years|          RENT|   50000.0|            NULL|       Not Verified|   112xx|        NY|     2429.0|    31577.0| 136879|        Fully Paid|    13725.0|   NY|      13725.0| 60 months|  0.1799|   18|   16 August 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidSXHgxZFJceDE2X...|Insurance Account...|  < 1 year|      MORTGAGE|   60000.0|        115000.0|       Not Verified|   218xx|        MD|    44723.0|   134170.0| 167757|           Current|    16000.0|   MD|      16000.0| 60 months|  0.1408|   14| 17 November 2017|     false| Joint App|debt_consolidation|  Debt consolidation|\n",
      "|YidnXHg4YX55XHhmN...|     Project manager|   2 years|      MORTGAGE|  120000.0|            NULL|    Source Verified|   633xx|        MO|    27296.0|   354842.0| 227799|        Fully Paid|    25000.0|   MO|      25000.0| 60 months|  0.1366|   14|  15 January 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDBiI1x4ODIrX...|       Night Stocker|   4 years|      MORTGAGE|   60000.0|            NULL|    Source Verified|   322xx|        FL|     6239.0|   155982.0|  98853|           Current|    10000.0|   FL|      10000.0| 36 months|     0.2|   16|      19 May 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicmXHhjN1x4OWNce...|               Buyer| 10+ years|          RENT|   56000.0|            NULL|    Source Verified|   018xx|        MA|     1888.0|    28319.0| 179547|       Charged Off|    18000.0|   MA|      18000.0| 60 months|  0.1049|    8|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicybFx4YWNceGI1X...|        truck driver|    1 year|          RENT|   32000.0|            NULL|       Not Verified|   973xx|        OR|     3644.0|    18218.0|   6780|        Fully Paid|     2400.0|   OR|       2400.0| 36 months|   0.202|   25|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidoXHhhYjZceGE3X...|  Nurse case manager|   3 years|      MORTGAGE|   70000.0|            NULL|           Verified|   270xx|        NC|    11394.0|   159521.0| 131937|           Current|    12800.0|   NC|      12800.0| 60 months|  0.1449|   12|     16 July 2016|     false|INDIVIDUAL|    major_purchase|      Major purchase|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions:  1\n"
     ]
    }
   ],
   "source": [
    "num_partitions = df.rdd.getNumPartitions()\n",
    "print(\"Number of partitions: \", num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logical cores: 12\n",
      "Physical cores: 10\n",
      "Number of partitions after repartitioning:  12\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "logical_cores = psutil.cpu_count(logical=True)  # Logical cores\n",
    "physical_cores = psutil.cpu_count(logical=False)  # Physical cores\n",
    "\n",
    "print(f\"Logical cores: {logical_cores}\")\n",
    "print(f\"Physical cores: {physical_cores}\")\n",
    "\n",
    "df_repartitioned = df.repartition(logical_cores)\n",
    "num_partitions = df_repartitioned.rdd.getNumPartitions()\n",
    "print(\"Number of partitions after repartitioning: \", num_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Rename All Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = df.columns\n",
    "\n",
    "new_columns = [col.replace(' ', '_').lower() for col in original_columns]\n",
    "\n",
    "df = df.toDF(*new_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|         customer_id|           emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|       loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|       issue_date|pymnt_plan|      type|           purpose|         description|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "|YidPVlx4ZTdceDkwN...|Medical Policy Nu...|   2 years|           OWN|   54000.0|            NULL|           Verified|   210xx|        MD|     5475.0|    54749.0|  69677|       Charged Off|     8000.0|   MD|       8000.0| 36 months|  0.2274|   21|    17 April 2017|     false|Individual|           medical|    Medical expenses|\n",
      "|YidceDAyXHgwMFx4Z...|              trller| 10+ years|          RENT|   37500.0|            NULL|    Source Verified|   988xx|        WA|     3478.0|    38257.0|  85890|           Current|    10000.0|   WA|      10000.0| 36 months|  0.0769|    3|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidBXHg5Ylx4ZmExe...|           Clinician|   6 years|      MORTGAGE|   72000.0|            NULL|    Source Verified|   088xx|        NJ|     2038.0|    34640.0| 149260|        Fully Paid|    15000.0|   NJ|      15000.0| 36 months|  0.0993|    9|  18 January 2018|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDBmXHg4YVx4Z...|Assistant US Atto...| 10+ years|          RENT|  158600.0|            NULL|    Source Verified|   021xx|        MA|     6129.0|    42904.0| 254638|           Current|    35000.0|   MA|      35000.0| 36 months|  0.0916|    7| 16 February 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YiJceDg2SlIwXHg3Z...|Account Represent...|   5 years|          RENT|   35000.0|            NULL|       Not Verified|   330xx|        FL|     1492.0|    28345.0|  80248|        Fully Paid|     9600.0|   FL|       9600.0| 36 months|  0.1498|   11| 13 December 2013|     false|INDIVIDUAL|debt_consolidation|  Debt Consolidation|\n",
      "|YidceGQ1Mlx4MDhce...|Lead Client Servi...|   8 years|          RENT|   35000.0|            NULL|    Source Verified|   011xx|        MA|     8524.0|    68190.0|  21975|        Fully Paid|     4500.0|   MA|       4500.0| 36 months|  0.0789|    3|  15 October 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGM4XHgwMlx4Y...|             Produce|   2 years|          RENT|   15000.0|         65000.0|    Source Verified|   723xx|        AR|     1011.0|     9100.0|  18997|           Current|     4000.0|   AR|       4000.0| 36 months|  0.1358|   12|    18 April 2018|     false| Joint App|debt_consolidation|  Debt consolidation|\n",
      "|YicuXHhlNFx4MGNce...|           Mechanic | 10+ years|          RENT|   80000.0|            NULL|    Source Verified|   107xx|        NY|     6639.0|    66387.0| 234661|           Current|    28000.0|   NY|      28000.0| 36 months|  0.0944|   10|  17 October 2017|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFkXHhkMVx4Z...|                  RN| 10+ years|          RENT|   90000.0|            NULL|       Not Verified|   023xx|        MA|     4308.0|    56009.0|  91392|Late (31-120 days)|    10000.0|   MA|      10000.0| 36 months|  0.1144|   10|    17 March 2017|     false|Individual|             other|               Other|\n",
      "|YidceGQ1WVx4YmVce...|Assistant Store M...|   6 years|          RENT|   65000.0|            NULL|    Source Verified|   109xx|        NY|     2237.0|    20135.0|  66884|           Current|     8000.0|   NY|       8000.0| 36 months|    NULL|   14| 16 November 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGUxXHhlNFx4Y...|Director of Facil...|   7 years|      MORTGAGE|  165000.0|            NULL|           Verified|   857xx|        AZ|    18621.0|   316554.0| 251397|           Current|    33100.0|   AZ|      33100.0| 36 months|  0.1367|   14|     16 June 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidceGFjXHg4YVx4M...|    Registered Nurse|   2 years|           OWN|   60000.0|            NULL|       Not Verified|   900xx|        CA|     3440.0|    51604.0| 124434|           Current|    12000.0|   CA|      12000.0| 60 months|  0.0899|    9|     16 July 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidqIUcmYihceDAyX...|        Truck driver|   3 years|          RENT|   80000.0|            NULL|       Not Verified|   752xx|        TX|     3569.0|    32122.0|  31146|           Current|     5000.0|   TX|       5000.0| 36 months|  0.1308|    9|   19 August 2019|     false|Individual|       credit_card|Credit card refin...|\n",
      "|Yic3XHhmNlx4Yzlce...|Guest Services As...|   2 years|          RENT|   50000.0|            NULL|       Not Verified|   112xx|        NY|     2429.0|    31577.0| 136879|        Fully Paid|    13725.0|   NY|      13725.0| 60 months|  0.1799|   18|   16 August 2016|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidSXHgxZFJceDE2X...|Insurance Account...|  < 1 year|      MORTGAGE|   60000.0|        115000.0|       Not Verified|   218xx|        MD|    44723.0|   134170.0| 167757|           Current|    16000.0|   MD|      16000.0| 60 months|  0.1408|   14| 17 November 2017|     false| Joint App|debt_consolidation|  Debt consolidation|\n",
      "|YidnXHg4YX55XHhmN...|     Project manager|   2 years|      MORTGAGE|  120000.0|            NULL|    Source Verified|   633xx|        MO|    27296.0|   354842.0| 227799|        Fully Paid|    25000.0|   MO|      25000.0| 60 months|  0.1366|   14|  15 January 2015|     false|Individual|       credit_card|Credit card refin...|\n",
      "|YidceDBiI1x4ODIrX...|       Night Stocker|   4 years|      MORTGAGE|   60000.0|            NULL|    Source Verified|   322xx|        FL|     6239.0|   155982.0|  98853|           Current|    10000.0|   FL|      10000.0| 36 months|     0.2|   16|      19 May 2019|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicmXHhjN1x4OWNce...|               Buyer| 10+ years|          RENT|   56000.0|            NULL|    Source Verified|   018xx|        MA|     1888.0|    28319.0| 179547|       Charged Off|    18000.0|   MA|      18000.0| 60 months|  0.1049|    8|  15 January 2015|     false|Individual|debt_consolidation|  Debt consolidation|\n",
      "|YicybFx4YWNceGI1X...|        truck driver|    1 year|          RENT|   32000.0|            NULL|       Not Verified|   973xx|        OR|     3644.0|    18218.0|   6780|        Fully Paid|     2400.0|   OR|       2400.0| 36 months|   0.202|   25|14 September 2014|     false|INDIVIDUAL|debt_consolidation|  Debt consolidation|\n",
      "|YidoXHhhYjZceGE3X...|  Nurse case manager|   3 years|      MORTGAGE|   70000.0|            NULL|           Verified|   270xx|        NC|    11394.0|   159521.0| 131937|           Current|    12800.0|   NC|      12800.0| 60 months|  0.1449|   12|     16 July 2016|     false|INDIVIDUAL|    major_purchase|      Major purchase|\n",
      "+--------------------+--------------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+------------------+-----------+-----+-------------+----------+--------+-----+-----------------+----------+----------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Detect Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def missing_percentage(df):\n",
    "    total_count = df.count()\n",
    "    missing_data = {}\n",
    "    \n",
    "    for column in df.columns:        \n",
    "        condition = isnull(col(column))\n",
    "        missing_count = df.filter(condition).count()\n",
    "        \n",
    "        missing_percentage = (missing_count / total_count) * 100\n",
    "        missing_data[column] = missing_percentage\n",
    "    \n",
    "    return missing_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data:\n",
      "customer_id: 0.00%\n",
      "emp_title: 8.68%\n",
      "emp_length: 6.93%\n",
      "home_ownership: 0.00%\n",
      "annual_inc: 0.00%\n",
      "annual_inc_joint: 93.15%\n",
      "verification_status: 0.00%\n",
      "zip_code: 0.00%\n",
      "addr_state: 0.00%\n",
      "avg_cur_bal: 0.00%\n",
      "tot_cur_bal: 0.00%\n",
      "loan_id: 0.00%\n",
      "loan_status: 0.00%\n",
      "loan_amount: 0.00%\n",
      "state: 0.00%\n",
      "funded_amount: 0.00%\n",
      "term: 0.00%\n",
      "int_rate: 4.48%\n",
      "grade: 0.00%\n",
      "issue_date: 0.00%\n",
      "pymnt_plan: 0.00%\n",
      "type: 0.00%\n",
      "purpose: 0.00%\n",
      "description: 0.79%\n"
     ]
    }
   ],
   "source": [
    "missing_info = missing_percentage(df)\n",
    "print(\"Missing data:\")\n",
    "for col_name, percentage in missing_info.items():\n",
    "    print(f\"{col_name}: {percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical and categorical columns\n",
    "numerical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, NumericType)]\n",
    "categorical_cols = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
    "\n",
    "df = df.fillna(0, subset=numerical_cols)\n",
    "\n",
    "mode_dict = {}\n",
    "for col_name in categorical_cols:\n",
    "\n",
    "    mode_row = (\n",
    "        df.filter(col(col_name).isNotNull())\n",
    "          .groupBy(col_name)\n",
    "          .count()\n",
    "          .orderBy(desc('count'))\n",
    "          .first()\n",
    "    )\n",
    "    mode_value = mode_row[col_name]\n",
    "    mode_dict[col_name] = mode_value\n",
    "\n",
    "df = df.fillna(value = mode_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data:\n",
      "customer_id: 0.00%\n",
      "emp_title: 0.00%\n",
      "emp_length: 0.00%\n",
      "home_ownership: 0.00%\n",
      "annual_inc: 0.00%\n",
      "annual_inc_joint: 0.00%\n",
      "verification_status: 0.00%\n",
      "zip_code: 0.00%\n",
      "addr_state: 0.00%\n",
      "avg_cur_bal: 0.00%\n",
      "tot_cur_bal: 0.00%\n",
      "loan_id: 0.00%\n",
      "loan_status: 0.00%\n",
      "loan_amount: 0.00%\n",
      "state: 0.00%\n",
      "funded_amount: 0.00%\n",
      "term: 0.00%\n",
      "int_rate: 0.00%\n",
      "grade: 0.00%\n",
      "issue_date: 0.00%\n",
      "pymnt_plan: 0.00%\n",
      "type: 0.00%\n",
      "purpose: 0.00%\n",
      "description: 0.00%\n"
     ]
    }
   ],
   "source": [
    "missing_info = missing_percentage(df)\n",
    "print(\"Missing data:\")\n",
    "for col_name, percentage in missing_info.items():\n",
    "    print(f\"{col_name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField('column_name', StringType(), True),\n",
    "    StructField('original_value', StringType(), True),\n",
    "    StructField('encoded_value', StringType(), True)\n",
    "])\n",
    "\n",
    "lookup = spark.createDataFrame([], schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'emp_length',\n",
    "    when(col('emp_length') == '< 1 year', 0.5)\n",
    "    .when(col('emp_length') == '10+ years', 10)\n",
    "    .otherwise(regexp_extract(col('emp_length'), r'(\\d+)', 1).cast(FloatType()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\n",
    "    'grade_letter',\n",
    "    when((col('grade') >= 1) & (col('grade') <= 5), 'A')\n",
    "    .when((col('grade') >= 6) & (col('grade') <= 10), 'B')\n",
    "    .when((col('grade') >= 11) & (col('grade') <= 15), 'C')\n",
    "    .when((col('grade') >= 16) & (col('grade') <= 20), 'D')\n",
    "    .when((col('grade') >= 21) & (col('grade') <= 25), 'E')\n",
    "    .when((col('grade') >= 26) & (col('grade') <= 30), 'F')\n",
    "    .when((col('grade') >= 31) & (col('grade') <= 35), 'G')\n",
    "    .otherwise('Unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+\n",
      "|column_name|original_value|encoded_value|\n",
      "+-----------+--------------+-------------+\n",
      "|      grade|             1|            A|\n",
      "|      grade|             2|            A|\n",
      "|      grade|             3|            A|\n",
      "|      grade|             4|            A|\n",
      "|      grade|             5|            A|\n",
      "|      grade|             6|            B|\n",
      "|      grade|             7|            B|\n",
      "|      grade|             8|            B|\n",
      "|      grade|             9|            B|\n",
      "|      grade|            10|            B|\n",
      "|      grade|            11|            C|\n",
      "|      grade|            12|            C|\n",
      "|      grade|            13|            C|\n",
      "|      grade|            14|            C|\n",
      "|      grade|            15|            C|\n",
      "|      grade|            16|            D|\n",
      "|      grade|            17|            D|\n",
      "|      grade|            18|            D|\n",
      "|      grade|            19|            D|\n",
      "|      grade|            20|            D|\n",
      "+-----------+--------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grade_mappings = []\n",
    "\n",
    "def map_grade(grade):\n",
    "    if 1 <= grade <= 5:\n",
    "        return 'A'\n",
    "    elif 6 <= grade <= 10:\n",
    "        return 'B'\n",
    "    elif 11 <= grade <= 15:\n",
    "        return 'C'\n",
    "    elif 16 <= grade <= 20:\n",
    "        return 'D'\n",
    "    elif 21 <= grade <= 25:\n",
    "        return 'E'\n",
    "    elif 26 <= grade <= 30:\n",
    "        return 'F'\n",
    "    elif 31 <= grade <= 35:\n",
    "        return 'G'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "    \n",
    "for i in range(1, 36):\n",
    "    letter = map_grade(i)\n",
    "    grade_mappings.append(('grade', str(i), letter))\n",
    "\n",
    "grade_lookup_df = spark.createDataFrame(grade_mappings, ['column_name', 'original_value', 'encoded_value'])\n",
    "\n",
    "lookup = lookup.union(grade_lookup_df)\n",
    "lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_indexer = StringIndexer(inputCol='state', outputCol='state_encoded', handleInvalid='keep')\n",
    "state_indexer_model = state_indexer.fit(df)\n",
    "\n",
    "purpose_indexer = StringIndexer(inputCol='purpose', outputCol='purpose_encoded', handleInvalid='keep')\n",
    "purpose_indexer_model = purpose_indexer.fit(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_labels = state_indexer_model.labels\n",
    "purpose_labels = purpose_indexer_model.labels\n",
    "\n",
    "state_mappings = [('state', label, float(index)) for index, label in enumerate(state_labels)]\n",
    "purpose_mappings = [('purpose', label, float(index)) for index, label in enumerate(purpose_labels)]\n",
    "\n",
    "state_lookup_df = spark.createDataFrame(state_mappings, ['column_name', 'original_value', 'encoded_value'])\n",
    "purpose_lookup_df = spark.createDataFrame(purpose_mappings, ['column_name', 'original_value', 'encoded_value'])\n",
    "\n",
    "lookup = lookup.union(state_lookup_df).union(purpose_lookup_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------------+\n",
      "|column_name|original_value|encoded_value|\n",
      "+-----------+--------------+-------------+\n",
      "|      grade|             1|            A|\n",
      "|      grade|             2|            A|\n",
      "|      grade|             3|            A|\n",
      "|      grade|             4|            A|\n",
      "|      grade|             5|            A|\n",
      "|      grade|             6|            B|\n",
      "|      grade|             7|            B|\n",
      "|      grade|             8|            B|\n",
      "|      grade|             9|            B|\n",
      "|      grade|            10|            B|\n",
      "|      grade|            11|            C|\n",
      "|      grade|            12|            C|\n",
      "|      grade|            13|            C|\n",
      "|      grade|            14|            C|\n",
      "|      grade|            15|            C|\n",
      "|      grade|            16|            D|\n",
      "|      grade|            17|            D|\n",
      "|      grade|            18|            D|\n",
      "|      grade|            19|            D|\n",
      "|      grade|            20|            D|\n",
      "|      grade|            21|            E|\n",
      "|      grade|            22|            E|\n",
      "|      grade|            23|            E|\n",
      "|      grade|            24|            E|\n",
      "|      grade|            25|            E|\n",
      "|      grade|            26|            F|\n",
      "|      grade|            27|            F|\n",
      "|      grade|            28|            F|\n",
      "|      grade|            29|            F|\n",
      "|      grade|            30|            F|\n",
      "|      grade|            31|            G|\n",
      "|      grade|            32|            G|\n",
      "|      grade|            33|            G|\n",
      "|      grade|            34|            G|\n",
      "|      grade|            35|            G|\n",
      "|      state|            CA|          0.0|\n",
      "|      state|            TX|          1.0|\n",
      "|      state|            NY|          2.0|\n",
      "|      state|            FL|          3.0|\n",
      "|      state|            IL|          4.0|\n",
      "|      state|            NJ|          5.0|\n",
      "|      state|            GA|          6.0|\n",
      "|      state|            PA|          7.0|\n",
      "|      state|            OH|          8.0|\n",
      "|      state|            VA|          9.0|\n",
      "|      state|            NC|         10.0|\n",
      "|      state|            MI|         11.0|\n",
      "|      state|            AZ|         12.0|\n",
      "|      state|            MD|         13.0|\n",
      "|      state|            MA|         14.0|\n",
      "+-----------+--------------+-------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lookup.show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of pipeline stages\n",
    "pipeline_stages = [\n",
    "    state_indexer,\n",
    "    purpose_indexer\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "df = pipeline.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home Ownership Categories: ['OWN', 'RENT', 'MORTGAGE', 'ANY']\n"
     ]
    }
   ],
   "source": [
    "home_ownership_categories = [row[0] for row in df.select('home_ownership').distinct().collect()]\n",
    "print(\"Home Ownership Categories:\", home_ownership_categories)\n",
    "for category in home_ownership_categories:\n",
    "    column_name = f'home_ownership_{category}'\n",
    "    df = df.withColumn(\n",
    "        column_name,\n",
    "        when(col('home_ownership') == category, 1).otherwise(0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Status Categories: ['Verified', 'Source Verified', 'Not Verified']\n"
     ]
    }
   ],
   "source": [
    "verification_status_categories = [row[0] for row in df.select('verification_status').distinct().collect()]\n",
    "print(\"Verification Status Categories:\", verification_status_categories)\n",
    "for category in verification_status_categories:\n",
    "    column_name = f'verification_status_{category}'\n",
    "    df = df.withColumn(\n",
    "        column_name,\n",
    "        when(col('verification_status') == category, 1).otherwise(0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type Categories: ['Joint App', 'Individual', 'DIRECT_PAY', 'JOINT', 'INDIVIDUAL']\n"
     ]
    }
   ],
   "source": [
    "type_categories = [row[0] for row in df.select('type').distinct().collect()]\n",
    "print(\"Type Categories:\", type_categories)\n",
    "for category in type_categories:\n",
    "    column_name = f'type_{category}'\n",
    "    df = df.withColumn(\n",
    "        column_name,\n",
    "        when(col('type') == category, 1).otherwise(0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------+--------------+------------------+-------------------+-----------------------+------------------+-------------------+----------------------------+-----------------------------------+--------------------------------+-----+-------------+----------+--------------+---------------+---------------+----------+---------------+------------------+---------------+\n",
      "|emp_length|grade|grade_letter|home_ownership|home_ownership_OWN|home_ownership_RENT|home_ownership_MORTGAGE|home_ownership_ANY|verification_status|verification_status_Verified|verification_status_Source Verified|verification_status_Not Verified|state|state_encoded|type      |type_Joint App|type_Individual|type_DIRECT_PAY|type_JOINT|type_INDIVIDUAL|purpose           |purpose_encoded|\n",
      "+----------+-----+------------+--------------+------------------+-------------------+-----------------------+------------------+-------------------+----------------------------+-----------------------------------+--------------------------------+-----+-------------+----------+--------------+---------------+---------------+----------+---------------+------------------+---------------+\n",
      "|2.0       |21   |E           |OWN           |1                 |0                  |0                      |0                 |Verified           |1                           |0                                  |0                               |MD   |13.0         |Individual|0             |0              |0              |0         |0              |medical           |5.0            |\n",
      "|10.0      |3    |A           |RENT          |0                 |1                  |0                      |0                 |Source Verified    |0                           |1                                  |0                               |WA   |16.0         |INDIVIDUAL|0             |1              |0              |0         |1              |debt_consolidation|0.0            |\n",
      "|6.0       |9    |B           |MORTGAGE      |0                 |0                  |1                      |0                 |Source Verified    |0                           |1                                  |0                               |NJ   |5.0          |Individual|0             |0              |0              |0         |0              |credit_card       |1.0            |\n",
      "|10.0      |7    |B           |RENT          |0                 |1                  |0                      |0                 |Source Verified    |0                           |1                                  |0                               |MA   |14.0         |INDIVIDUAL|0             |1              |0              |0         |1              |debt_consolidation|0.0            |\n",
      "|5.0       |11   |C           |RENT          |0                 |1                  |0                      |0                 |Not Verified       |0                           |0                                  |1                               |FL   |3.0          |INDIVIDUAL|0             |1              |0              |0         |1              |debt_consolidation|0.0            |\n",
      "+----------+-----+------------+--------------+------------------+-------------------+-----------------------+------------------+-------------------+----------------------------+-----------------------------------+--------------------------------+-----+-------------+----------+--------------+---------------+---------------+----------+---------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_columns = (\n",
    "    ['emp_length', 'grade', 'grade_letter'] +\n",
    "    ['home_ownership'] + [f'home_ownership_{cat}' for cat in home_ownership_categories] +\n",
    "    ['verification_status'] + [f'verification_status_{cat}' for cat in verification_status_categories] +\n",
    "    ['state', 'state_encoded'] +\n",
    "    ['type'] + [f'type_{cat}' for cat in type_categories] +\n",
    "    ['purpose', 'purpose_encoded']\n",
    ")\n",
    "\n",
    "df.select(*selected_columns).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous_loan_features(df):\n",
    "    df = df.withColumn('issue_date', to_date(col('issue_date'), 'dd MMMM yyyy'))\n",
    "\n",
    "\n",
    "    grade_window = Window.partitionBy('grade').orderBy('issue_date')\n",
    "    state_grade_window = Window.partitionBy('state', 'grade').orderBy('issue_date')\n",
    "\n",
    "    # Compute previous loan features\n",
    "    df = df.withColumn('prev_issue_date_same_grade', lag('issue_date').over(grade_window))\n",
    "    df = df.withColumn('prev_loan_amount_same_grade', lag('loan_amount').over(grade_window))\n",
    "    df = df.withColumn('prev_issue_date_same_state_grade', lag('issue_date').over(state_grade_window))\n",
    "    df = df.withColumn('prev_loan_amount_same_state_grade', lag('loan_amount').over(state_grade_window))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+-----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "|issue_date|grade|state|loan_amount|prev_issue_date_same_grade|prev_loan_amount_same_grade|prev_issue_date_same_state_grade|prev_loan_amount_same_state_grade|\n",
      "+----------+-----+-----+-----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "|2017-11-17|    1|   AK|    21000.0|                2017-11-17|                    12000.0|                            NULL|                             NULL|\n",
      "|2018-08-18|    1|   AK|    35000.0|                2018-08-18|                     4000.0|                      2017-11-17|                          21000.0|\n",
      "|2019-01-19|    1|   AK|    10000.0|                2019-01-19|                    16000.0|                      2018-08-18|                          35000.0|\n",
      "|2017-04-17|    2|   AK|    30000.0|                2017-04-17|                    35000.0|                            NULL|                             NULL|\n",
      "|2017-06-17|    2|   AK|    30000.0|                2017-06-17|                     6000.0|                      2017-04-17|                          30000.0|\n",
      "|2019-02-19|    2|   AK|    10000.0|                2019-02-19|                    40000.0|                      2017-06-17|                          30000.0|\n",
      "|2019-07-19|    2|   AK|    16850.0|                2019-07-19|                    26000.0|                      2019-02-19|                          10000.0|\n",
      "|2018-09-18|    3|   AK|    14000.0|                2018-09-18|                     8000.0|                            NULL|                             NULL|\n",
      "|2019-04-19|    3|   AK|    40000.0|                2019-04-19|                     3875.0|                      2018-09-18|                          14000.0|\n",
      "|2015-06-15|    4|   AK|    12000.0|                2015-05-15|                     8000.0|                            NULL|                             NULL|\n",
      "+----------+-----+-----+-----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = add_previous_loan_features(df)\n",
    "\n",
    "df.select(\n",
    "    'issue_date', 'grade', 'state', 'loan_amount',\n",
    "    'prev_issue_date_same_grade', 'prev_loan_amount_same_grade',\n",
    "    'prev_issue_date_same_state_grade', 'prev_loan_amount_same_state_grade'\n",
    ").show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Analysis SQL vs Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Identify the average loan amount and interest rate for loans marked as\n",
    "\"Default\" in the Loan Status, grouped by Emp Length and annual income ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+------------------+------------+-------------+---------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "|         customer_id|      emp_title|emp_length|home_ownership|annual_inc|annual_inc_joint|verification_status|zip_code|addr_state|avg_cur_bal|tot_cur_bal|loan_id|loan_status|loan_amount|state|funded_amount|      term|int_rate|grade|issue_date|pymnt_plan|      type|           purpose|       description|grade_letter|state_encoded|purpose_encoded|home_ownership_OWN|home_ownership_RENT|home_ownership_MORTGAGE|home_ownership_ANY|verification_status_Verified|verification_status_Source Verified|verification_status_Not Verified|type_Joint App|type_INDIVIDUAL|type_DIRECT_PAY|type_JOINT|prev_issue_date_same_grade|prev_loan_amount_same_grade|prev_issue_date_same_state_grade|prev_loan_amount_same_state_grade|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+------------------+------------+-------------+---------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "|YidceGI1JWRqSmJiX...|Finance Manager|       8.0|          RENT|   32000.0|             0.0|    Source Verified|   999xx|        AK|     1187.0|    11873.0|  72193|Charged Off|     8500.0|   AK|       8500.0| 36 months|  0.0818|    6|2015-03-15|     false|Individual|debt_consolidation|Debt consolidation|           B|         43.0|            0.0|                 0|                  1|                      0|                 0|                           0|                                  1|                               0|             0|              0|              0|         0|                2015-03-15|                    15000.0|                            NULL|                             NULL|\n",
      "|Yid6XHhmOEN6c1x4Z...|     CDL Fueler|       3.0|      MORTGAGE|   85000.0|             0.0|       Not Verified|   996xx|        AK|    12204.0|   109835.0| 209296| Fully Paid|    22000.0|   AK|      22000.0| 36 months|  0.0818|    6|2015-09-15|     false|Individual|debt_consolidation|Debt consolidation|           B|         43.0|            0.0|                 0|                  0|                      1|                 0|                           0|                                  0|                               1|             0|              0|              0|         0|                2015-08-15|                    12000.0|                      2015-03-15|                           8500.0|\n",
      "+--------------------+---------------+----------+--------------+----------+----------------+-------------------+--------+----------+-----------+-----------+-------+-----------+-----------+-----+-------------+----------+--------+-----+----------+----------+----------+------------------+------------------+------------+-------------+---------------+------------------+-------------------+-----------------------+------------------+----------------------------+-----------------------------------+--------------------------------+--------------+---------------+---------------+----------+--------------------------+---------------------------+--------------------------------+---------------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Loan Status Categories: ['Fully Paid', 'In Grace Period', 'Charged Off', 'Late (31-120 days)', 'Current', 'Late (16-30 days)']\n"
     ]
    }
   ],
   "source": [
    "df.show(2)\n",
    "loan_status_categories = [row[0] for row in df.select('loan_status').distinct().collect()]\n",
    "print(\"Loan Status Categories:\", loan_status_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+-------------------+\n",
      "|emp_length|   income_range|   avg_loan_amount|       avg_int_rate|\n",
      "+----------+---------------+------------------+-------------------+\n",
      "|       8.0|   $80k - $100k|16931.052631578947| 0.1269084210526316|\n",
      "|       2.0|    $40k - $60k| 12700.37688442211|0.12375603015075379|\n",
      "|       4.0|    $40k - $60k|    12542.08984375|0.13204453125000004|\n",
      "|       3.0|    $40k - $60k|11908.090185676392|0.12477055702917764|\n",
      "|       9.0|$100k and above|23534.601449275364| 0.1145789855072464|\n",
      "|       0.5|    $60k - $80k| 16628.10650887574|0.12550857988165676|\n",
      "|       8.0|    $20k - $40k| 10079.62962962963|0.12387592592592592|\n",
      "|       7.0|$100k and above|21381.969696969696|0.11129090909090919|\n",
      "|       8.0|    $40k - $60k|13156.612903225807| 0.1329161290322581|\n",
      "|       5.0|   $80k - $100k|18131.676136363636| 0.1242278409090909|\n",
      "|       5.0|$100k and above|22477.911646586344| 0.1163646586345382|\n",
      "|       1.0|    $40k - $60k|11677.083333333334|0.12718966666666667|\n",
      "|       2.0|    $60k - $80k|       16021.40625|         0.12517375|\n",
      "|       4.0|     Under $20k| 9240.384615384615|0.15594615384615385|\n",
      "|       5.0|    $60k - $80k|15398.863636363636|0.12223681818181827|\n",
      "|       6.0|$100k and above|22144.337016574587|0.11826132596685095|\n",
      "|       9.0|    $20k - $40k| 8268.442622950819|0.13998196721311482|\n",
      "|       0.5|$100k and above|23099.545454545456|  0.110645974025974|\n",
      "|       5.0|     Under $20k|  9359.09090909091| 0.1871090909090909|\n",
      "|       7.0|   $80k - $100k| 18958.01282051282|0.11672692307692305|\n",
      "+----------+---------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"fintech\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    emp_length,\n",
    "    CASE\n",
    "        WHEN annual_inc < 20000 THEN 'Under $20k'\n",
    "        WHEN annual_inc >= 20000 AND annual_inc < 40000 THEN '$20k - $40k'\n",
    "        WHEN annual_inc >= 40000 AND annual_inc < 60000 THEN '$40k - $60k'\n",
    "        WHEN annual_inc >= 60000 AND annual_inc < 80000 THEN '$60k - $80k'\n",
    "        WHEN annual_inc >= 80000 AND annual_inc < 100000 THEN '$80k - $100k'\n",
    "        ELSE '$100k and above'\n",
    "    END AS income_range,\n",
    "    AVG(loan_amount) AS avg_loan_amount,\n",
    "    AVG(int_rate) AS avg_int_rate\n",
    "FROM\n",
    "    fintech\n",
    "WHERE\n",
    "    loan_status = 'Current'\n",
    "GROUP BY\n",
    "    emp_length,\n",
    "    income_range\n",
    "\"\"\"\n",
    "\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+------------------+-------------------+\n",
      "|emp_length|   income_range|   avg_loan_amount|       avg_int_rate|\n",
      "+----------+---------------+------------------+-------------------+\n",
      "|       8.0|   $80k - $100k|16931.052631578947| 0.1269084210526316|\n",
      "|       2.0|    $40k - $60k| 12700.37688442211|0.12375603015075379|\n",
      "|       4.0|    $40k - $60k|    12542.08984375|0.13204453125000004|\n",
      "|       3.0|    $40k - $60k|11908.090185676392|0.12477055702917764|\n",
      "|       9.0|$100k and above|23534.601449275364| 0.1145789855072464|\n",
      "|       0.5|    $60k - $80k| 16628.10650887574|0.12550857988165676|\n",
      "|       8.0|    $20k - $40k| 10079.62962962963|0.12387592592592592|\n",
      "|       7.0|$100k and above|21381.969696969696|0.11129090909090919|\n",
      "|       8.0|    $40k - $60k|13156.612903225807| 0.1329161290322581|\n",
      "|       5.0|   $80k - $100k|18131.676136363636| 0.1242278409090909|\n",
      "|       5.0|$100k and above|22477.911646586344| 0.1163646586345382|\n",
      "|       1.0|    $40k - $60k|11677.083333333334|0.12718966666666667|\n",
      "|       2.0|    $60k - $80k|       16021.40625|         0.12517375|\n",
      "|       4.0|     Under $20k| 9240.384615384615|0.15594615384615385|\n",
      "|       5.0|    $60k - $80k|15398.863636363636|0.12223681818181827|\n",
      "|       6.0|$100k and above|22144.337016574587|0.11826132596685095|\n",
      "|       9.0|    $20k - $40k| 8268.442622950819|0.13998196721311482|\n",
      "|       0.5|$100k and above|23099.545454545456|  0.110645974025974|\n",
      "|       5.0|     Under $20k|  9359.09090909091| 0.1871090909090909|\n",
      "|       7.0|   $80k - $100k| 18958.01282051282|0.11672692307692305|\n",
      "+----------+---------------+------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_current = df.filter(df.loan_status == 'Current')\n",
    "\n",
    "df_current = df_current.withColumn(\n",
    "    'income_range',\n",
    "    when(df.annual_inc < 20000, 'Under $20k')\n",
    "    .when((df.annual_inc >= 20000) & (df.annual_inc < 40000), '$20k - $40k')\n",
    "    .when((df.annual_inc >= 40000) & (df.annual_inc < 60000), '$40k - $60k')\n",
    "    .when((df.annual_inc >= 60000) & (df.annual_inc < 80000), '$60k - $80k')\n",
    "    .when((df.annual_inc >= 80000) & (df.annual_inc < 100000), '$80k - $100k')\n",
    "    .otherwise('$100k and above')\n",
    ")\n",
    "\n",
    "result = df_current.groupBy('emp_length', 'income_range').agg(\n",
    "    avg('loan_amount').alias('avg_loan_amount'),\n",
    "    avg('int_rate').alias('avg_int_rate')\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Calculate the average difference between Loan Amount and Funded Amount for each\n",
    "loan Grade and sort by the grades with the largest differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|grade|avg_diff|\n",
      "+-----+--------+\n",
      "|   26|     0.0|\n",
      "|   29|     0.0|\n",
      "|   19|     0.0|\n",
      "|   22|     0.0|\n",
      "|    7|     0.0|\n",
      "|   34|     0.0|\n",
      "|   32|     0.0|\n",
      "|   31|     0.0|\n",
      "|   25|     0.0|\n",
      "|    6|     0.0|\n",
      "|    9|     0.0|\n",
      "|   27|     0.0|\n",
      "|   17|     0.0|\n",
      "|   28|     0.0|\n",
      "|   33|     0.0|\n",
      "|    5|     0.0|\n",
      "|    1|     0.0|\n",
      "|   10|     0.0|\n",
      "|    3|     0.0|\n",
      "|   12|     0.0|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    grade,\n",
    "    AVG(loan_amount - funded_amount) AS avg_diff\n",
    "FROM\n",
    "    fintech\n",
    "GROUP BY\n",
    "    grade\n",
    "ORDER BY\n",
    "    avg_diff DESC\n",
    "\"\"\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|grade|avg_diff|\n",
      "+-----+--------+\n",
      "|   26|     0.0|\n",
      "|   29|     0.0|\n",
      "|   19|     0.0|\n",
      "|   22|     0.0|\n",
      "|    7|     0.0|\n",
      "|   34|     0.0|\n",
      "|   32|     0.0|\n",
      "|   31|     0.0|\n",
      "|   25|     0.0|\n",
      "|    6|     0.0|\n",
      "|    9|     0.0|\n",
      "|   27|     0.0|\n",
      "|   17|     0.0|\n",
      "|   28|     0.0|\n",
      "|   33|     0.0|\n",
      "|    5|     0.0|\n",
      "|    1|     0.0|\n",
      "|   10|     0.0|\n",
      "|    3|     0.0|\n",
      "|   12|     0.0|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df.groupBy('grade') \\\n",
    "    .agg(avg(col('loan_amount') - col('funded_amount')).alias('avg_diff')) \\\n",
    "    .orderBy(col('avg_diff').desc())\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the total Loan Amount for loans with \"Verified\" and \"Not Verified\"\n",
    "Verification Status across each state (Addr State).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+------------------+\n",
      "|addr_state|total_verified|total_not_verified|\n",
      "+----------+--------------+------------------+\n",
      "|        AZ|     2732350.0|         2761125.0|\n",
      "|        SC|     1385325.0|         1549875.0|\n",
      "|        LA|     1442725.0|          904850.0|\n",
      "|        MN|     1642100.0|         2179400.0|\n",
      "|        NJ|     4472550.0|         4470675.0|\n",
      "|        DC|      252850.0|          171400.0|\n",
      "|        OR|     1375500.0|         1742500.0|\n",
      "|        VA|     3724325.0|         4232675.0|\n",
      "|        RI|      510675.0|          488400.0|\n",
      "|        WY|      363600.0|          205075.0|\n",
      "|        KY|      871800.0|         1207050.0|\n",
      "|        NH|      480175.0|          788725.0|\n",
      "|        MI|     2970650.0|         3393675.0|\n",
      "|        NV|     2077775.0|         2109600.0|\n",
      "|        WI|     1622475.0|         1823100.0|\n",
      "|        ID|      271900.0|          256475.0|\n",
      "|        CA|     1.66511E7|       1.8556975E7|\n",
      "|        CT|     1624875.0|         2336375.0|\n",
      "|        NE|      297200.0|          344125.0|\n",
      "|        MT|      356725.0|          326150.0|\n",
      "+----------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    addr_state,\n",
    "    SUM(CASE WHEN verification_status = 'Verified' THEN loan_amount ELSE 0 END) AS total_verified,\n",
    "    SUM(CASE WHEN verification_status = 'Not Verified' THEN loan_amount ELSE 0 END) AS total_not_verified\n",
    "FROM\n",
    "    fintech\n",
    "GROUP BY\n",
    "    addr_state\n",
    "\"\"\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----------------+\n",
      "|addr_state|verified_status|total_loan_amount|\n",
      "+----------+---------------+-----------------+\n",
      "|        AK|   Not Verified|         377800.0|\n",
      "|        AK|       Verified|         447375.0|\n",
      "|        AL|   Not Verified|        1529775.0|\n",
      "|        AL|       Verified|        1464600.0|\n",
      "|        AR|   Not Verified|         961100.0|\n",
      "|        AR|       Verified|         799400.0|\n",
      "|        AZ|   Not Verified|        2761125.0|\n",
      "|        AZ|       Verified|        2732350.0|\n",
      "|        CA|   Not Verified|      1.8556975E7|\n",
      "|        CA|       Verified|        1.66511E7|\n",
      "|        CO|       Verified|        2388425.0|\n",
      "|        CO|   Not Verified|        2852300.0|\n",
      "|        CT|   Not Verified|        2336375.0|\n",
      "|        CT|       Verified|        1624875.0|\n",
      "|        DC|       Verified|         252850.0|\n",
      "|        DC|   Not Verified|         171400.0|\n",
      "|        DE|       Verified|         345325.0|\n",
      "|        DE|   Not Verified|         475500.0|\n",
      "|        FL|   Not Verified|        9622125.0|\n",
      "|        FL|       Verified|        7795750.0|\n",
      "+----------+---------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.select('addr_state', 'verification_status', 'loan_amount')\n",
    "\n",
    "df_filtered = df_filtered.withColumn('verified_status',\n",
    "    when(col('verification_status') == 'Verified', 'Verified')\n",
    "    .when(col('verification_status') == 'Not Verified', 'Not Verified')\n",
    "    .otherwise('Other')\n",
    ")\n",
    "\n",
    "df_filtered = df_filtered.filter(col('verified_status') != 'Other')\n",
    "\n",
    "df_grouped = df_filtered.groupBy('addr_state', 'verified_status') \\\n",
    "    .agg(sum('loan_amount').alias('total_loan_amount')) \\\n",
    "    .orderBy('addr_state')\n",
    "\n",
    "df_grouped.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the average time gap (in days) between consecutive loans for each\n",
    "grade using the new features you added in the feature engineering phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|grade| avg_time_gap_days|\n",
      "+-----+------------------+\n",
      "|    1|2.3186528497409324|\n",
      "|    2|2.2664389410760033|\n",
      "|    3|2.4171220400728597|\n",
      "|    4| 2.228215767634855|\n",
      "|    5|2.2800687285223367|\n",
      "|    6|1.6016898008449003|\n",
      "|    7|1.7222578576010263|\n",
      "|    8|1.7067524115755628|\n",
      "|    9|1.7503259452411994|\n",
      "|   10|1.6656327543424319|\n",
      "|   11|1.7884097035040432|\n",
      "|   12|1.6958466453674121|\n",
      "|   13|1.7769688947716744|\n",
      "|   14| 1.855563234277816|\n",
      "|   15| 1.789615643964936|\n",
      "|   16| 3.325814536340852|\n",
      "|   17|  3.39386189258312|\n",
      "|   18| 3.437900128040973|\n",
      "|   19| 3.446753246753247|\n",
      "|   20|3.3354037267080745|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    grade,\n",
    "    AVG(DATEDIFF(issue_date, prev_issue_date_same_grade)) AS avg_time_gap_days\n",
    "FROM\n",
    "    fintech\n",
    "WHERE\n",
    "    prev_issue_date_same_grade IS NOT NULL\n",
    "GROUP BY\n",
    "    grade\n",
    "\"\"\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|grade| avg_time_gap_days|\n",
      "+-----+------------------+\n",
      "|    1|2.3186528497409324|\n",
      "|    2|2.2664389410760033|\n",
      "|    3|2.4171220400728597|\n",
      "|    4| 2.228215767634855|\n",
      "|    5|2.2800687285223367|\n",
      "|    6|1.6016898008449003|\n",
      "|    7|1.7222578576010263|\n",
      "|    8|1.7067524115755628|\n",
      "|    9|1.7503259452411994|\n",
      "|   10|1.6656327543424319|\n",
      "|   11|1.7884097035040432|\n",
      "|   12|1.6958466453674121|\n",
      "|   13|1.7769688947716744|\n",
      "|   14| 1.855563234277816|\n",
      "|   15| 1.789615643964936|\n",
      "|   16| 3.325814536340852|\n",
      "|   17|  3.39386189258312|\n",
      "|   18| 3.437900128040973|\n",
      "|   19| 3.446753246753247|\n",
      "|   20|3.3354037267080745|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, datediff, avg\n",
    "\n",
    "df_filtered = df.filter(col('prev_issue_date_same_grade').isNotNull())\n",
    "\n",
    "df_with_gap = df_filtered.withColumn(\n",
    "    'time_gap_days',\n",
    "    datediff(col('issue_date'), col('prev_issue_date_same_grade'))\n",
    ")\n",
    "\n",
    "result = df_with_gap.groupBy('grade').agg(\n",
    "    avg('time_gap_days').alias('avg_time_gap_days')\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Identify the average difference in loan amounts between consecutive loans\n",
    "within the same state and grade combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------------------+\n",
      "|state|grade|avg_diff_in_loan_amounts|\n",
      "+-----+-----+------------------------+\n",
      "|   AK|    1|                 -5500.0|\n",
      "|   AK|    2|      -4383.333333333333|\n",
      "|   AK|    3|                 26000.0|\n",
      "|   AK|    4|                  8400.0|\n",
      "|   AK|    5|                  2387.5|\n",
      "|   AK|    6|                  1250.0|\n",
      "|   AK|    7|                  7000.0|\n",
      "|   AK|    8|                  1937.5|\n",
      "|   AK|    9|                     0.0|\n",
      "|   AK|   10|                     0.0|\n",
      "|   AK|   11|                  -875.0|\n",
      "|   AK|   12|                 17000.0|\n",
      "|   AK|   14|     -3466.6666666666665|\n",
      "|   AK|   15|      -5333.333333333333|\n",
      "|   AK|   16|                  9475.0|\n",
      "|   AK|   17|                  2300.0|\n",
      "|   AK|   19|                   275.0|\n",
      "|   AK|   20|                 -4100.0|\n",
      "|   AL|    1|     -1333.3333333333333|\n",
      "|   AL|    2|      2555.5555555555557|\n",
      "+-----+-----+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    state,\n",
    "    grade,\n",
    "    AVG(loan_amount - prev_loan_amount_same_state_grade) AS avg_diff_in_loan_amounts\n",
    "FROM\n",
    "    fintech\n",
    "WHERE\n",
    "    prev_loan_amount_same_state_grade IS NOT NULL\n",
    "GROUP BY\n",
    "    state,\n",
    "    grade\n",
    "\"\"\"\n",
    "result_df = spark.sql(query)\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------------------+\n",
      "|state|grade|avg_loan_amount_diff|\n",
      "+-----+-----+--------------------+\n",
      "|   AK|    1|             -5500.0|\n",
      "|   AK|    2|  -4383.333333333333|\n",
      "|   AK|    3|             26000.0|\n",
      "|   AK|    4|              8400.0|\n",
      "|   AK|    5|              2387.5|\n",
      "|   AK|    6|              1250.0|\n",
      "|   AK|    7|              7000.0|\n",
      "|   AK|    8|              1937.5|\n",
      "|   AK|    9|                 0.0|\n",
      "|   AK|   10|                 0.0|\n",
      "|   AK|   11|              -875.0|\n",
      "|   AK|   12|             17000.0|\n",
      "|   AK|   14| -3466.6666666666665|\n",
      "|   AK|   15|  -5333.333333333333|\n",
      "|   AK|   16|              9475.0|\n",
      "|   AK|   17|              2300.0|\n",
      "|   AK|   19|               275.0|\n",
      "|   AK|   20|             -4100.0|\n",
      "|   AL|    1| -1333.3333333333333|\n",
      "|   AL|    2|  2555.5555555555557|\n",
      "+-----+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter(col('prev_loan_amount_same_state_grade').isNotNull())\n",
    "\n",
    "df_with_diff = df_filtered.withColumn(\n",
    "    'loan_amount_diff',\n",
    "    col('loan_amount') - col('prev_loan_amount_same_state_grade')\n",
    ")\n",
    "\n",
    "result = df_with_diff.groupBy('state', 'grade').agg(\n",
    "    avg('loan_amount_diff').alias('avg_loan_amount_diff')\n",
    ")\n",
    "\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Lookup Table & Saving the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(1)\n",
    "lookup = lookup.repartition(1)\n",
    "\n",
    "df.write.parquet(\"./fintech_spark_52_0812_clean.parquet\", mode=\"overwrite\")\n",
    "lookup.write.parquet(\"./lookup_spark_52_0812.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Database\n",
      "Writing dataset to database\n",
      "Table already exists.\n",
      "Connected to Database\n",
      "Writing dataset to database\n",
      "Table already exists.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/testdb')\n",
    "\n",
    "def save_to_db(cleaned, name):\n",
    "    if(engine.connect()):\n",
    "        print('Connected to Database')\n",
    "        try:\n",
    "            cleaned = cleaned.toPandas()\n",
    "            print('Writing dataset to database')\n",
    "            cleaned.to_sql(name, con=engine, if_exists='fail')\n",
    "            print('Done writing to database')\n",
    "        except ValueError as vx:\n",
    "            print('Table already exists.')\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "    else:\n",
    "        print('Failed to connect to Database')\n",
    "\n",
    "save_to_db(df,'cleaned_fintech')\n",
    "save_to_db(lookup,'lookup_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
