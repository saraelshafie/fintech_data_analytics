{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 - EDA and Preprocessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Important note*** - This is merely a template. you are recommended to create your own notebook from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Make sure to include markdown-based text commenting and explaining each step you perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = pd.read_csv(data_dir + 'fintech_data_43_52_0812.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying up column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
    "\n",
    "clean_column_names(fintech_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df.info()\n",
    "fintech_df_copy = fintech_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "*Do customers with higher income tend to opt for shorter or longer loan terms?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Convert 'term' to integers (remove ' months' and convert to int)\n",
    "fintech_df_copy['term_int'] = fintech_df_copy['term'].str.replace(' months', '').astype(int)\n",
    "\n",
    "# Log-transform annual income (add a small constant to avoid log(0) issues)\n",
    "fintech_df_copy['log_annual_inc'] = np.log1p(fintech_df_copy['annual_inc'])\n",
    "\n",
    "\n",
    "# KDE plot for income distribution across loan terms\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.kdeplot(data=fintech_df_copy[fintech_df_copy['term_int'] == 36], x='log_annual_inc', label='36 Months', fill=True)\n",
    "sns.kdeplot(data=fintech_df_copy[fintech_df_copy['term_int'] == 60], x='log_annual_inc', label='60 Months', fill=True)\n",
    "plt.title('Income Distribution for Different Loan Terms')\n",
    "plt.xlabel('Annual Income')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights from the Plot**:\n",
    "1. Both the 36-month and 60-month loan options have similar income distributions, with most customers having annual incomes around the same range. The peak (mode) of both curves is near the same income value, suggesting that most customers, regardless of loan term, have annual incomes in a similar range.\n",
    "2. The 60-month loan term has a slightly higher density at the peak, indicating that more customers with average annual incomes (around the mode) choose the longer loan term.\n",
    "4. Both curves taper off at higher incomes (right side of the graph). This shows that as income increases beyond a certain threshold, the preference for loan terms does not vary significantly.\n",
    "Conclusion:\n",
    "The general pattern shows a broad overlap between the two groups, with more customers opting for 60-month loans at average income levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "*What are the most common reasons for applying for loans (Purpose), and how does the interest rate (Int Rate) differ across these purposes?Can we identify which purpose has the lowest interest rate?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the unique values of the 'purpose' column\n",
    "unique_purposes = fintech_df_copy['purpose'].unique()\n",
    "print(unique_purposes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the most common loan purposes\n",
    "top_purposes = fintech_df_copy['purpose'].value_counts().nlargest(10)  # Get top 10 most common purposes\n",
    "\n",
    "# Filter the dataframe for only the most common purposes\n",
    "filtered_df = fintech_df_copy[fintech_df_copy['purpose'].isin(top_purposes.index)]\n",
    "\n",
    "# Plot interest rate distribution for different loan purposes\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='purpose', y='int_rate', data=filtered_df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Interest Rate Differences Across Loan Purposes')\n",
    "plt.xlabel('Loan Purpose')\n",
    "plt.ylabel('Interest Rate (%)')\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average interest rate for each loan purpose\n",
    "avg_interest_by_purpose = fintech_df_copy.groupby('purpose')['int_rate'].mean().reset_index()\n",
    "\n",
    "# Find the purpose with the lowest average interest rate\n",
    "lowest_int_rate_purpose = avg_interest_by_purpose.loc[avg_interest_by_purpose['int_rate'].idxmin()]\n",
    "\n",
    "# Output the result\n",
    "print(\"Loan purpose with the lowest interest rate:\")\n",
    "print(lowest_int_rate_purpose)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights from the Plot**:\n",
    "\n",
    "1. **credit_card** purpose has the **lowest median interest rate**, making it more favorable for borrowers in terms of cost.\n",
    "\n",
    "2. **Small_business** loans tend to have the **highest median interest rate**, which might reflect the higher risk associated with business loans.\n",
    "\n",
    "3. **Small_business** and **major_purchase** loans show wider variability in interest rates, indicating that interest rates for these purposes can vary greatly depending on the borrowerâ€™s profile.\n",
    "\n",
    "**Conclusion:**\n",
    "credit card-related loans have the most favorable interest rates, while small business loans are typically more expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "*Which states have the highest proportion of risky loans (loans graded lower, such as D, E, or F), and how does this correlate with the likelihood of default or late payment (Loan Status)? Can we identify geographic areas that might require stricter lending criteria?*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the unique values of the 'purpose' column\n",
    "unique_status = fintech_df_copy['loan_status'].unique()\n",
    "print(unique_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the numeric grades to their corresponding letter grades\n",
    "def map_grade(numeric_grade):\n",
    "    if 1 <= numeric_grade <= 5:\n",
    "        return 'A'\n",
    "    elif 6 <= numeric_grade <= 10:\n",
    "        return 'B'\n",
    "    elif 11 <= numeric_grade <= 15:\n",
    "        return 'C'\n",
    "    elif 16 <= numeric_grade <= 20:\n",
    "        return 'D'\n",
    "    elif 21 <= numeric_grade <= 25:\n",
    "        return 'E'\n",
    "    elif 26 <= numeric_grade <= 30:\n",
    "        return 'F'\n",
    "    elif 31 <= numeric_grade <= 35:\n",
    "        return 'G'\n",
    "\n",
    "fintech_df_copy['letter_grade'] = fintech_df_copy['grade'].apply(map_grade)\n",
    "\n",
    "# Filter the data to include only risky loans (grades D, E, or F)\n",
    "risky_grades = ['D', 'E', 'F', 'G']\n",
    "risky_loans_df = fintech_df_copy[fintech_df_copy['letter_grade'].isin(risky_grades)]\n",
    "\n",
    "# Calculate the proportion of risky loans by state\n",
    "risky_loans_by_state = risky_loans_df.groupby('state').size() / fintech_df_copy.groupby('state').size()\n",
    "risky_loans_by_state = risky_loans_by_state.dropna()\n",
    "\n",
    "# Plot the proportion of risky loans by state\n",
    "plt.figure(figsize=(12,8))\n",
    "risky_loans_by_state.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Proportion of Risky Loans (Grades D, E, F, G) by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Proportion of Risky Loans')\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlation between loan status (\"Charged Off\" or \"Late\") and risky grades by state\n",
    "risky_status_df = risky_loans_df[risky_loans_df['loan_status'].isin(['Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'])]\n",
    "\n",
    "# Calculate proportion of risky loans that are \"Charged Off\" or \"Late\" by state\n",
    "risky_status_by_state = risky_status_df.groupby('state').size() / risky_loans_df.groupby('state').size()\n",
    "risky_status_by_state = risky_status_by_state.dropna()\n",
    "\n",
    "# Plot the proportion of risky loans with default/late payment by state\n",
    "plt.figure(figsize=(12,8))\n",
    "risky_status_by_state.sort_values(ascending=False).plot(kind='bar', color='red')\n",
    "plt.title('Proportion of Risky Loans (Grades D, E, F, G) with Default or Late Payment by State')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Proportion of Default/Late Payments')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights from the Plots:**\n",
    "\n",
    "1. **Proportion of Risky Loans (First Plot)**:\n",
    "   - **West Virginia (WV)** and **Washington D.C. (DC)** have the **highest proportion** of risky loans (grades D, E, F, G).\n",
    "   - **North Dakota (ND)** has the **lowest proportion** of risky loans.\n",
    "\n",
    "2. **Risk of Default or Late Payment (Second Plot)**:\n",
    "   - **South Dakota (SD)** and **Kansas (KS)** have the **highest proportion** of risky loans that result in **default or late payment**.\n",
    "   - States like **New Hampshire (NH)** and **Montana (MT)** have the **lowest default/late payment rates** for risky loans.\n",
    "\n",
    "**Conclusion:**\n",
    "- States like **WV** and **DC** have a high concentration of risky loans, but **SD** and **KS** show the highest likelihood of default or late payment. These states may benefit from **stricter lending criteria** to mitigate risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "*Are customers in payment plans more likely to have loans that are \"Current\" or \"Late\"?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by payment plan status and loan status\n",
    "payment_plan_status = fintech_df_copy.groupby(['pymnt_plan', 'loan_status']).size().unstack()\n",
    "\n",
    "# Normalize the data by the total number of customers in each group (convert to proportions)\n",
    "payment_plan_status = payment_plan_status.div(payment_plan_status.sum(axis=1), axis=0)\n",
    "\n",
    "# Plot the proportion of loan statuses for customers in payment plans vs not\n",
    "payment_plan_status[['Current', 'Late (31-120 days)', 'Late (16-30 days)']].plot(kind='bar', stacked=True, figsize=(10, 6), color=['green', 'red', 'orange'])\n",
    "plt.title('Proportion of Loan Status (\"Current\" vs \"Late\") by Payment Plan Status')\n",
    "plt.xlabel('Payment Plan Status')\n",
    "plt.ylabel('Proportion of Loans')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Loan Status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights from the Graph:**\n",
    "\n",
    "1. **Customers Not in Payment Plans**:\n",
    "   - A **majority of loans** are **current**.\n",
    "   - Only a **small proportion** of loans are **late**.\n",
    "\n",
    "2. **Customers in Payment Plans**:\n",
    "   - A **significant portion of loans** are either **late 31-120 days** or **late 16-30 days**.\n",
    "   - **No loans** are **current** for customers in payment plans.\n",
    "\n",
    "**Conclusion:** Customers in payment plans are **much more likely to have loans that are late**, whereas customers not in payment plans are **more likely to have current loans**.This suggests that being in a payment plan correlates with a higher likelihood of delayed payments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "*Are individual borrowers more likely to experience late payments as interest rates increase compared to joint borrowers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_unique = fintech_df_copy['type'].unique()\n",
    "print(type_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the 'type' field to categorize borrowers\n",
    "fintech_df_copy['borrower_type'] = fintech_df_copy['type'].replace({\n",
    "    'Individual': 'Individual',\n",
    "    'INDIVIDUAL': 'Individual',\n",
    "    'Joint App': 'Joint',\n",
    "    'JOINT': 'Joint',\n",
    "    'DIRECT_PAY': 'Direct Pay'  # If you want to handle Direct Pay separately\n",
    "})\n",
    "\n",
    "# Filter for relevant loan statuses (Charged Off, Late)\n",
    "loan_status_filtered = fintech_df_copy[fintech_df_copy['loan_status'].isin(['Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'])]\n",
    "\n",
    "# Separate individual and joint borrowers\n",
    "individual_borrowers = loan_status_filtered[fintech_df_copy['borrower_type'] == 'Individual']\n",
    "joint_borrowers = loan_status_filtered[fintech_df_copy['borrower_type'] == 'Joint']\n",
    "\n",
    "# Plot interest rates vs loan status for individual borrowers\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='loan_status', y='int_rate', data=individual_borrowers)\n",
    "plt.title('Interest Rate vs Loan Status (Individual Borrowers)')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Interest Rate')\n",
    "plt.show()\n",
    "\n",
    "# Plot interest rates vs loan status for joint borrowers\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='loan_status', y='int_rate', data=joint_borrowers)\n",
    "plt.title('Interest Rate vs Loan Status (Joint Borrowers)')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Interest Rate')\n",
    "plt.show()\n",
    "\n",
    "# Compare the default and late payment rates for individual vs joint borrowers\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(individual_borrowers['int_rate'], label='Individual Borrowers', color='blue', kde=True)\n",
    "sns.histplot(joint_borrowers['int_rate'], label='Joint Borrowers', color='orange', kde=True)\n",
    "plt.title('Interest Rate Distribution for Late Loans (Individual vs Joint Borrowers)')\n",
    "plt.xlabel('Interest Rate')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights:**\n",
    "\n",
    "1. **Individual Borrowers**: The **interest rates** for **\"Charged Off\"** loans tend to be slightly higher compared to late payments (16-30 days and 31-120 days).\n",
    "\n",
    "2. **Joint Borrowers**: The overall interest rates for joint borrowers across statuses (Late or Charged Off) tend to cluster around higher values than individual borrowers.\n",
    "\n",
    "3. **Comparison**:\n",
    "   - The histogram shows that **individual borrowers** tend to be late across a **wider range of interest rates**, especially in the mid-range (0.12 - 0.20).\n",
    "   - **Joint borrowers**, by contrast, seem to have a **tighter distribution** with higher interest rates but fewer overall late payments, suggesting **joint borrowers may be less prone to late payments** at the same interest rates compared to individuals.\n",
    "\n",
    "**Conclusion:**\n",
    "- **Individual borrowers** are more likely to experience late payments over a wider range of interest rates.\n",
    "- **Joint borrowers**, although charged slightly higher interest rates, tend to pay late less frequently than individual borrowers, especially in the mid-interest range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "*Does the length of employment (Emp Length) correlate with loan default risk (Loan Status)? Do borrowers with longer employment histories receive better loan grades?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_length_unique = fintech_df_copy['emp_length'].unique()\n",
    "print(emp_length_unique)\n",
    "\n",
    "# Clean 'emp_length' to ensure it's numeric\n",
    "emp_length_mapping = {\n",
    "    '10+ years': 10,\n",
    "    '< 1 year': 0.5,\n",
    "    '1 year': 1,\n",
    "    '2 years': 2,\n",
    "    '3 years': 3,\n",
    "    '4 years': 4,\n",
    "    '5 years': 5,\n",
    "    '6 years': 6,\n",
    "    '7 years': 7,\n",
    "    '8 years': 8,\n",
    "    '9 years': 9,\n",
    "    'n/a': None\n",
    "}\n",
    "fintech_df_copy['emp_length_clean'] = fintech_df_copy['emp_length'].replace(emp_length_mapping)\n",
    "\n",
    "# Part 1: Analyze Loan Default Risk by Employment Length\n",
    "\n",
    "# Print unique loan status values to see the exact names\n",
    "loan_status_unique = fintech_df_copy['loan_status'].unique()\n",
    "print(loan_status_unique)\n",
    "\n",
    "# Filter for relevant loan statuses (Charged Off, Late, Fully Paid)\n",
    "loan_status_filtered = fintech_df_copy[fintech_df_copy['loan_status'].isin(['Fully Paid', 'Charged Off', 'Late (31-120 days)', 'Late (16-30 days)'])]\n",
    "\n",
    "# Plot the distribution of loan statuses across employment length\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='loan_status', y='emp_length_clean', data=loan_status_filtered)\n",
    "plt.title('Employment Length vs Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Employment Length (Years)')\n",
    "plt.show()\n",
    "\n",
    "# Part 2: Analyze Loan Grades by Employment Length\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(x='letter_grade', y='emp_length_clean', data=fintech_df_copy)\n",
    "plt.title('Employment Length vs Loan Grade')\n",
    "plt.xlabel('Loan Grade')\n",
    "plt.ylabel('Employment Length (Years)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Key Insights**\n",
    "\n",
    "**Insights from Employment Length vs Loan Status:**\n",
    "Borrowers with **Late (16-30 days)** tend to have slightly **shorter employment lengths**, but overall, there is no drastic difference between late payments and the other categories.\n",
    "\n",
    "**Insights from Employment Length vs Loan Grade:**\n",
    "   - Employment length appears to be fairly consistent across loan grades (A to G).\n",
    "   - Borrowers with longer employment do not necessarily receive better grades, as the **median employment length** is similar across all grades.\n",
    "\n",
    "**Conclusion:** Longer employment does not significantly reduce the risk of loan defaults or late payments. Employment length does not appear to heavily influence the loan grade assigned to a borrower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are cleaned at the beginning to facilitate easier exploratory data analysis (EDA) on the dataset. Next, the `customer_id` column, which is unique for each customer, will be set as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_df_index(df, index_col):\n",
    "    df = df.set_index(index_col, inplace=True)\n",
    "    return df\n",
    "\n",
    "set_df_index(fintech_df, 'customer_id')\n",
    "fintech_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe inconsistent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_values(fintech_df):\n",
    "    # Loop through non-numeric columns and print the unique values for each\n",
    "    for column in fintech_df.select_dtypes(exclude=['float64']).columns:\n",
    "        unique_values = fintech_df[column].unique()\n",
    "        print(f\"Unique values in '{column}':\")\n",
    "        print(unique_values)\n",
    "        print(\"\\n\")\n",
    "\n",
    "print_unique_values(fintech_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation of Inconsistent Data:\n",
    "`type` Field (Inconsistent Capitalization):\n",
    "There are duplicate representations of the same value, such as 'Individual' and 'INDIVIDUAL', and similarly, 'Joint App' and 'JOINT'.\n",
    "Action: Normalize the values to ensure consistent representation.\n",
    "\n",
    "`emp_length` Field (Inconsistent Representations):\n",
    "The values '10+ years', '2 years', and '< 1 year' represent employment length in different formats.\n",
    "Action: Standardize the employment length field by converting these into numeric values (e.g., '10+ years' â†’ 10, '< 1 year' â†’ 0.5).\n",
    "\n",
    "`home_ownership` Field:\n",
    "There is an unusual value 'ANY', which could be considered irrelevant or a data entry error, as it doesn't seem to align with traditional categories like 'OWN', 'RENT', and 'MORTGAGE'.\n",
    "Action: Investigate further to determine if 'ANY' is valid or should be removed/recategorized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_type_field(df):\n",
    "    df['type'] = df['type'].replace({\n",
    "        'Individual': 'Individual',\n",
    "        'INDIVIDUAL': 'Individual',\n",
    "        'Joint App': 'Joint',\n",
    "        'JOINT': 'Joint',\n",
    "        'DIRECT_PAY': 'Direct Pay'\n",
    "    })\n",
    "\n",
    "normalize_type_field(fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_emp_length(df):\n",
    "    emp_length_mapping = {\n",
    "        '10+ years': 10,\n",
    "        '< 1 year': 0.5,\n",
    "        '1 year': 1,\n",
    "        '2 years': 2,\n",
    "        '3 years': 3,\n",
    "        '4 years': 4,\n",
    "        '5 years': 5,\n",
    "        '6 years': 6,\n",
    "        '7 years': 7,\n",
    "        '8 years': 8,\n",
    "        '9 years': 9,\n",
    "        'n/a': None\n",
    "    }\n",
    "    df['emp_length'] = df['emp_length'].replace(emp_length_mapping)\n",
    "\n",
    "clean_emp_length(fintech_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df[fintech_df['home_ownership'] == 'ANY'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the data, 'ANY' seems to be a valid value in the `home_ownership` field, so we leave it as is without replacing or dropping it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process involves normalizing, merging, and standardizing values for consistency, which will improve the quality and reliability of any analysis performed on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Lookup Dataframe to use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_df():\n",
    "    lookup_df = pd.DataFrame(columns=['column', 'original', 'imputed'])\n",
    "    return lookup_df\n",
    "\n",
    "lookup_df = create_lookup_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lookup_values(lookup_df, column_name, original_column, encoded_column):\n",
    "    unique_values = original_column.unique()\n",
    "    unique_encoded_values = encoded_column.unique()\n",
    "    new_rows = pd.DataFrame({\n",
    "        'column': column_name,\n",
    "        'original': unique_values,\n",
    "        'imputed': unique_encoded_values,\n",
    "    })\n",
    "    lookup_df = pd.concat([lookup_df, new_rows], ignore_index=True)\n",
    "\n",
    "    return lookup_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perc_null = fintech_df.isnull().mean() * 100\n",
    "print(\"Percentage of Missing Values in Each Column:\")\n",
    "perc_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only columns that are missing are `annual_inc_joint`, `emp_title`, `emp_length`, `int_rate`, `description`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fintech_df[fintech_df['annual_inc_joint'].isnull()]['type'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `annual_inc_joint` is missing when the type is either 'Individual' or 'Direct Pay'. This makes sense because joint income is only relevant for joint borrowers, and thus this field should not have values when the borrower type is not 'Joint'. \n",
    "\n",
    "----\n",
    "\n",
    "Now we move on to observe if there is a pattern in the missingness of `emp_title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_copy = fintech_df.copy()\n",
    "\n",
    "# Step 1: Create bins for annual income to group the salary into ranges\n",
    "salary_bins = [0, 25000, 50000, 75000, 100000, 150000, 200000, 300000, fintech_df['annual_inc'].max()]\n",
    "salary_labels = ['<25k', '25k-50k', '50k-75k', '75k-100k', '100k-150k', '150k-200k', '200k-300k', '>300k']\n",
    "fintech_df_copy['income_range'] = pd.cut(fintech_df['annual_inc'], bins=salary_bins, labels=salary_labels)\n",
    "\n",
    "# Step 2: Create a new column to indicate whether emp_title is missing\n",
    "fintech_df_copy['emp_title_missing'] = fintech_df_copy['emp_title'].isna()\n",
    "\n",
    "# Step 3: Plot the proportion of missing emp_title in each salary range\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='income_range', y='emp_title_missing', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Employee Title by Salary Range')\n",
    "plt.xlabel('Annual Income Range')\n",
    "plt.ylabel('Proportion of Missing Employee Title')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missingness of `emp_title` is likely **MAR**, as it is related to the salary of the borrowers. Based on the plot, the missing values for the `emp_title` column seem to be concentrated in the lower salary ranges, particularly in the <25k range. The logical explanation for this pattern could be due to individuals in lower salary ranges being employed in unstable or informal jobs where titles are less defined (e.g., part-time jobs, contract work, or temporary positions). As a result, they may be less likely to report a formal job title because they either do not have one or the job title does not seem relevant or prestigious enough to report.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to observe if there is a pattern in the missingness of `emp_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Investigate relationship with salary (annual_inc)\n",
    "\n",
    "# Create a column to indicate if emp_length is missing\n",
    "fintech_df_copy['emp_length_missing'] = fintech_df_copy['emp_length'].isna()\n",
    "\n",
    "# SVisualize the proportion of missing emp_length by income range\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='income_range', y='emp_length_missing', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Employment Length by Salary Range')\n",
    "plt.xlabel('Annual Income Range')\n",
    "plt.ylabel('Proportion of Missing Employment Length')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Investigate relationship with employment title (emp_title)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='emp_title_missing', y='emp_length_missing', data=fintech_df_copy)\n",
    "plt.title('Missing Employment Length vs Missing Employment Title')\n",
    "plt.xlabel('Is Employment Title Missing?')\n",
    "plt.ylabel('Proportion of Missing Employment Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missingness of `emp_length` seems to follow a **MAR** pattern based on the visualizations. \n",
    "From the first plot, we can see that a higher proportion of missing `emp_length` occurs in the lower salary ranges, particularly in the <25k range. People in low-paying jobs might have unstable or irregular employment histories, making them less likely to report their employment length. These borrowers may have short-term jobs, part-time roles, or be employed in positions where disclosing employment tenure is less common or relevant. Borrowers with lower incomes may also avoid providing detailed employment information because they perceive it as not important or because they are self-employed without a formal job duration.\n",
    "\n",
    "The second plot shows that when the `emp_title` is missing, the emp_length is always missing as well.\n",
    "This is likely because both fields are closely related: if a borrower does not provide their job title (perhaps due to informal or unstable work), it makes sense that they would also omit their employment length. People who leave their job title blank may not want to disclose their job history either.\n",
    "\n",
    "In conclusion, the missingness of emp_length is **MAR** because it appears to be dependent on observed variables, like low income and missing job title. The logical reasoning is that borrowers in lower salary ranges, or those with informal/unstable jobs, are less likely to provide employment information (both title and length).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to observe if there is a pattern in the missingness of `int_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in 'int_rate'\n",
    "missing_int_rate = fintech_df['int_rate'].isna()\n",
    "\n",
    "fintech_df_copy['missing_int_rate'] = missing_int_rate\n",
    "\n",
    "# Step 1: Visualize missing int_rate by loan term\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='term', y='missing_int_rate', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Interest Rate by Loan Term')\n",
    "plt.xlabel('Loan Term (Months)')\n",
    "plt.ylabel('Proportion of Missing Interest Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Visualize missing int_rate by loan status\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='loan_status', y='missing_int_rate', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Interest Rate by Loan Status')\n",
    "plt.xlabel('Loan Status')\n",
    "plt.ylabel('Proportion of Missing Interest Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "fintech_df_copy['letter_grade'] = fintech_df_copy['grade'].apply(map_grade)\n",
    "\n",
    "# Step 3: Visualize missing int_rate by loan grade\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='letter_grade', y='missing_int_rate', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Interest Rate by Loan Grade')\n",
    "plt.xlabel('Loan Grade')\n",
    "plt.ylabel('Proportion of Missing Interest Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Visualize missing int_rate by funded amount range\n",
    "# Create bins for funded amount\n",
    "# Checking the maximum value of funded_amount\n",
    "max_funded = fintech_df_copy['funded_amount'].max()\n",
    "# Set bins based on the maximum value of funded_amount\n",
    "if max_funded < 50000:\n",
    "    funded_bins = [0, 5000, 10000, 20000, max_funded]\n",
    "    funded_labels = ['<5k', '5k-10k', '10k-20k', f'20k-{int(max_funded)}k']\n",
    "else:\n",
    "    funded_bins = [0, 5000, 10000, 20000, 50000, max_funded]\n",
    "    funded_labels = ['<5k', '5k-10k', '10k-20k', '20k-50k', f'>50k']\n",
    "\n",
    "# Now apply the binning safely\n",
    "fintech_df_copy['funded_range'] = pd.cut(fintech_df_copy['funded_amount'], bins=funded_bins, labels=funded_labels, include_lowest=True)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='funded_range', y='missing_int_rate', data=fintech_df_copy, estimator=lambda x: sum(x) / len(x))\n",
    "plt.title('Proportion of Missing Interest Rate by Funded Amount Range')\n",
    "plt.xlabel('Funded Amount Range')\n",
    "plt.ylabel('Proportion of Missing Interest Rate')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing `int_rate` values are likely **MCAR** because no clear pattern or correlation was observed between the missingness and other variables in the dataset (such as loan term, loan grade, or funded amount). This suggests that the missing interest rates are unrelated to both observed data and the actual interest rate values. They were likely missed due to random chance, possibly during the data entry process, such as when filling out the form.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to observe if there is a pattern in the missingness of `description`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values in the `description` field are likely **MCAR**. Since this field is **optional**, borrowers can choose whether or not to provide a description, and the missingness is not systematically related to other variables. It likely occurs due to borrower preference, without any underlying pattern or bias in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic univariate imputation function\n",
    "def univariate_imputation(df, column, fill_value):\n",
    "    df[column].fillna(fill_value, inplace=True)\n",
    "\n",
    "# Generic multivariate imputation function\n",
    "def multivariate_imputation(df, column_to_impute, group_by_column, method='mode'):\n",
    "    if method == 'mode':\n",
    "        df[column_to_impute] = df.groupby(group_by_column)[column_to_impute].transform(\n",
    "            lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown')\n",
    "        )\n",
    "    elif method == 'median':\n",
    "        global_median = df[column_to_impute].median()  # Calculate global median\n",
    "        df[column_to_impute] = df.groupby(group_by_column)[column_to_impute].transform(\n",
    "            lambda x: x.fillna(x.median() if not x.dropna().empty else global_median)\n",
    "        )\n",
    "\n",
    "def null_values_sum(df,column):\n",
    "    return df[column].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`annual_inc_joint`: Since the missing values are logically tied to borrower type, we do not need to impute these values because it makes sense for them to be missing.\n",
    "\n",
    "We can simply replace the null values with 0 to avoid any null values disrupting the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_lookup_df(lookup_df, column_name, original_value, imputed_value):\n",
    "    lookup_df = pd.concat([lookup_df, pd.DataFrame([{'column': column_name, 'original': original_value, 'imputed': imputed_value}])], ignore_index=True)\n",
    "    return lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_imputation(fintech_df, 'annual_inc_joint', 0)\n",
    "lookup_df = update_lookup_df(lookup_df, 'annual_inc_joint', 'missing', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_sum(fintech_df,'annual_inc_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emp_title` and `emp_length`: These are both MAR based on the salary ranges. We can impute these missing values ussing the median or mode based on salary ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_emp_fields(df):\n",
    "    # Define salary ranges without creating a new column\n",
    "    salary_bins = [0, 25000, 50000, 75000, 100000, 150000, 200000, 300000, df['annual_inc'].max()]\n",
    "    salary_labels = ['<25k', '25k-50k', '50k-75k', '75k-100k', '100k-150k', '150k-200k', '200k-300k', '>300k']\n",
    "    salary_groups = pd.cut(df['annual_inc'], bins=salary_bins, labels=salary_labels)\n",
    "\n",
    "    # Impute 'emp_title' using mode within salary ranges\n",
    "    multivariate_imputation(df, 'emp_title', salary_groups, method='mode')\n",
    "\n",
    "    # Impute 'emp_length' using median within salary ranges\n",
    "    multivariate_imputation(df, 'emp_length', salary_groups, method='median')\n",
    "\n",
    "impute_emp_fields(fintech_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_sum(fintech_df,'emp_title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_sum(fintech_df,'emp_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`int_rate`: Since the missing values are MCAR, simple imputation methods like mean or median can be used without introducing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = fintech_df['int_rate'].mean()\n",
    "univariate_imputation(fintech_df, 'int_rate', mean)\n",
    "lookup_df = update_lookup_df(lookup_df, 'int_rate', 'missing', mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_sum(fintech_df,'int_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`description`: The missing values are also MCAR, meaning they were omitted optionally by the borrowers. Since the description is not a critical feature for numeric processing, we can simply fill missing values with a placeholder such as 'No Description'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_imputation(fintech_df, 'description', 'No Description')\n",
    "lookup_df = update_lookup_df(lookup_df, 'description', 'missing', 'No Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_sum(fintech_df,'description')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers(df, col, method='Z-Score', threshold=3):\n",
    "    \n",
    "    if method == 'Z-Score':\n",
    "        z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "        df['z_score'] = z_scores\n",
    "        z_outliers_mask = df['z_score'] > threshold\n",
    "        df.drop(columns='z_score', inplace=True)\n",
    "        outliers = df[z_outliers_mask] \n",
    "    elif method == 'IQR':\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        print(f'Outliers below: {Q1 - 1.5 * IQR:.3f}')\n",
    "        print(f'Outliers above: {Q3 + 1.5 * IQR:.3f}')\n",
    "        \n",
    "        iqr_outliers_mask = (df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))\n",
    "        outliers = df[iqr_outliers_mask]\n",
    "\n",
    "    print(f'Percentage of Outliers: {len(outliers)/len(df)*100:.3f}%')\n",
    "\n",
    "        \n",
    "    return outliers\n",
    "\n",
    "def plot_distribution(df,col):\n",
    "    sns.histplot(df[col], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = fintech_df.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(f\"Numeric Columns: {numeric_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with `emp_length` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'emp_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `emp_length` appears to be **right-skewed** based on the plot, with a long tail on the right side, especially toward the 10+ years range.\n",
    "\n",
    "Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'emp_length', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis using the IQR method for detecting outliers in `emp_length`, it was found that **0% of the data points were outliers**. This indicates that despite the skewness observed in the distribution, the values of `emp_length` do not fall outside the acceptable range for outliers using the IQR criterion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `annual_inc` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'annual_inc')\n",
    "plt.xlim(0, 800000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `annual_inc` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = detect_outliers(fintech_df, 'annual_inc', method='IQR')\n",
    "len(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `annual_inc` column outliers below -25,000 and above 167,000 were identified, accounting for 5.18% of the data points.\n",
    "\n",
    "The outliers above 167,000 likely represent high-income earners whose annual income is significantly above the typical range of the dataset.\n",
    "The outliers below -25,000 could be due to incorrect or erroneous data entries, as negative annual incomes do not make practical sense.\n",
    "This percentage suggests that a moderate portion of the data falls outside the expected range for annual income. Depending on the analysis goals, these outliers need to be addressed to avoid skewing the results.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `annual_inc_joint` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'annual_inc_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `annual_inc_joint` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'annual_inc_joint', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `annual_inc_joint` column, the outliers detected below **0.000** and above **0.000** account for **6.85%** of the data. However, it's important to note that approximately **94%** of this column was originally null, and we imputed these missing values with **0** because it made sense to do so for non-Joint borrowers.\n",
    "\n",
    "Therefore, the **6.85%** of data points flagged as outliers are not actually outliers in the true sense. Instead, they represent legitimate cases where the `annual_inc_joint` field was appropriately filled for borrowers who applied for Joint loans. These values are not problematic and should not be treated as outliers. This reinforces that the imputation decision was valid for this context.\n",
    "\n",
    "Let's observe if there is any outliers in the values that are not equal to **0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = fintech_df[fintech_df['annual_inc_joint'] > 0]\n",
    "\n",
    "plot_distribution(filtered_df,'annual_inc_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `annual_inc_joint` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(filtered_df, 'annual_inc_joint', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outliers for `annual_inc_joint` values greater than zero fall below -15,375 and above 265,225, representing 4.268% of the non-zero data. Given the nature of this field and its relation to joint loans, these outliers likely represent extreme or uncommon cases in borrower income, which could be considered for further transformation to reduce their impact.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `avg_cur_bal` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'avg_cur_bal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `avg_cur_bal` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'avg_cur_bal', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `avg_cur_bal` column, we detected outliers as follows:\n",
    "\n",
    "- **Outliers below -20,667.75** and above **42,820.25**: The presence of outliers below a negative value (which is not possible for a balance) may indicate errors in the data, as balances typically cannot be negative beyond certain limits, and especially not to such a large extent.\n",
    "- **5.516%** of the data is flagged as outliers, suggesting that a notable proportion of customers have significantly higher or lower average current balances compared to the majority.\n",
    "\n",
    "The high percentage of outliers could indicate that balances in this dataset are spread across a wide range, with certain customers having extremely high or low balances.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `tot_cur_bal` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'tot_cur_bal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `tot_cur_bal` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'tot_cur_bal', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `tot_cur_bal` column, we detected outliers as follows:\n",
    "- **3.43%** of the data is identified as outliers, which means a small but significant portion of the data has either very high or very low total balances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `loan_amount` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'loan_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `loan_amount` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'loan_amount', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `loan_amount` column, we detected the following outliers:\n",
    "\n",
    "- **Outliers below -10,890.63** and above **39,484.38**: These thresholds suggest that any loan amount outside this range is considered an outlier. While negative loan amounts are not plausible in real-world scenarios.\n",
    "\n",
    "- **2.44%** of the data is flagged as outliers, which is a relatively small percentage. However, given the nature of loan data, it is important to carefully review any loans outside this range, particularly the negative values, as they may distort any financial analysis or modeling.\n",
    "\n",
    "This percentage is manageable, but it's critical to address these outliers to ensure the accuracy and integrity of the dataset for further analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `funded_amount` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'funded_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `funded_amount` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'funded_amount', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `funded_amount` column, we detected the following outliers:\n",
    "\n",
    "- **Outliers below -10,890.63** and above **39,484.38**: These thresholds suggest that any loan amount outside this range is considered an outlier. While negative loan amounts are not plausible in real-world scenarios.\n",
    "\n",
    "- **2.44%** of the data is flagged as outliers, which is a relatively small percentage. However, given the nature of loan data, it is important to carefully review any loans outside this range, particularly the negative values, as they may distort any financial analysis or modeling.\n",
    "\n",
    "This percentage is manageable, but it's critical to address these outliers to ensure the accuracy and integrity of the dataset for further analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's observe `int_rate` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution(fintech_df,'int_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of `int_rate` appears to be **right-skewed** based on the plot. Given that the data is skewed and does not follow a normal distribution, IQR would be the more appropriate method for detecting outliers in this case. Z-score would not be suitable since it assumes a normal distribution, which is not true for this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_outliers(fintech_df, 'int_rate', method='IQR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `int_rate` column, the following outliers were detected:\n",
    "\n",
    "- **Outliers below 0.00% and above 25.3%**: This indicates that any interest rates exceeding 25.3% are considered outliers. Since 0% interest rates are unlikely in most lending situations, any value at or below 0% is also flagged, possibly indicating erroneous entries or special cases.\n",
    "\n",
    "- **2.29%** of the interest rate data is flagged as outliers. This is a small proportion of the overall data, which suggests that the majority of interest rates fall within a reasonable range, but these outliers could represent either high-risk loans, special loan agreements, or errors.\n",
    "\n",
    "Itâ€™s essential to review these outliers to verify if they are valid cases or if any corrections need to be made, especially for values near 0%, which may skew analysis if left unaddressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `annual_inc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log transformation is a common technique used to manage outliers, especially in datasets that are positively skewed. Therefore we will apply the log transform on the `annual_inc` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function for log transformation\n",
    "def get_log_transformation(df, column):\n",
    "    return np.log(df[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_distributions(df, col, new_col):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "    sns.histplot(df[col],ax=ax[0], kde=True);\n",
    "    ax[0].set_title('Original Data');\n",
    "\n",
    "    sns.histplot(new_col, ax=ax[1], kde=True);\n",
    "    ax[1].set_title('Outliers Handled Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_annual_inc = get_log_transformation(fintech_df, 'annual_inc')\n",
    "compare_distributions(fintech_df, 'annual_inc', log_annual_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_transformation(df, col, transformed_col, ignore_zero=False):\n",
    "    if ignore_zero:\n",
    "        df.loc[df[col] != 0, col] = transformed_col\n",
    "    else:\n",
    "        df[col] = transformed_col\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_transformation(fintech_df, 'annual_inc', log_annual_inc)\n",
    "outliers = detect_outliers(fintech_df, 'annual_inc', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the log transformation to `annual_inc`, the distribution became more normal, reducing outliers from **5.18%** to **1.95%**. This shows the transformation effectively addressed skewness and minimized extreme values.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `annual_inc_joint` values > 0\n",
    "\n",
    "We will apply the log transform as it is positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = fintech_df[fintech_df['annual_inc_joint'] != 0]\n",
    "log_annual_inc_joint = get_log_transformation(filtered_df, 'annual_inc_joint')\n",
    "compare_distributions(filtered_df, 'annual_inc_joint', log_annual_inc_joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'annual_inc_joint', log_annual_inc_joint, ignore_zero=True)\n",
    "outliers = detect_outliers(fintech_df[fintech_df['annual_inc_joint'] != 0], 'annual_inc_joint', method='IQR')\n",
    "len(outliers)\n",
    "\n",
    "outliers = detect_outliers(fintech_df[fintech_df['annual_inc_joint'] != 0], 'annual_inc_joint', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the log transformation to `annual_inc_joint` values > 0 and handling the outliers, the percentage of outliers has decreased to 1.405%, with outliers occurring below 10.540 and above 12.848. This shows a significant improvement in reducing the impact of extreme values.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `avg_cur_bal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the log transform on the `avg_cur_bal` column as it is positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_avg_cur_bal = get_log_transformation(fintech_df, 'avg_cur_bal')\n",
    "compare_distributions(fintech_df, 'avg_cur_bal', log_avg_cur_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'avg_cur_bal', log_avg_cur_bal)\n",
    "outliers = detect_outliers(fintech_df, 'avg_cur_bal', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the log transformation to `avg_cur_bal`, the outliers decreased significantly from **5.52%** to **0.31%**, and the distribution became much closer to normal. This demonstrates the transformation's effectiveness in reducing skewness and outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `tot_cur_bal`\n",
    "\n",
    "We will apply the log transform on the `tot_cur_bal` column as it is positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_tot_cur_bal = get_log_transformation(fintech_df, 'tot_cur_bal')\n",
    "compare_distributions(fintech_df, 'tot_cur_bal', log_tot_cur_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'tot_cur_bal', log_tot_cur_bal)\n",
    "outliers = detect_outliers(fintech_df, 'tot_cur_bal', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the log transformation to `tot_cur_bal`, the outliers decreased significantly from **3.43%** to **0.425%**, and the distribution became much closer to normal. This demonstrates the transformation's effectiveness in reducing skewness and outliers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `loan_amount`\n",
    "\n",
    "We will apply the log transform on the `loan_amount` column as it is positively skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loan_amount = get_log_transformation(fintech_df, 'loan_amount')\n",
    "compare_distributions(fintech_df, 'loan_amount', log_loan_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the log transformation decreased the number of outliers for the `loan_amount`, it increased the left skewness of the distribution. Therefore, we will use the capping method to handle the outliers more effectively without distorting the distribution.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers(df, column):\n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    \n",
    "    # Calculate IQR (Interquartile Range)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Calculate the lower and upper bounds for capping\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    print(f'Lower Bound: {lower_bound:.3f}')\n",
    "    print(f'Upper Bound: {upper_bound:.3f}')\n",
    "    \n",
    "    cap_column = np.where(df[column] < lower_bound, lower_bound, \n",
    "                          np.where(df[column] > upper_bound, upper_bound, df[column]))\n",
    "    \n",
    "    return cap_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_loan_amount = cap_outliers(fintech_df, 'loan_amount')\n",
    "compare_distributions(fintech_df, 'loan_amount', cap_loan_amount)\n",
    "fintech_df['loan_amount'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'loan_amount', cap_loan_amount)\n",
    "outliers = detect_outliers(fintech_df, 'loan_amount', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `loan_amount` column initially contained outliers that were either below -10,890.625 or above 39,484.375, with 2.438% of the data being considered outliers. After applying the capping method, these outliers were removed, bringing the percentage of outliers down to 0%. Additionally, the distribution after capping remains similar to the original, as the extreme values were handled without drastically altering the overall shape of the data distribution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `funded_amount`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `funded_amount` column had the exact same outlier range and IQR limits as the loan amount and a similar percentage of outliers. Therefore, we will apply the same capping method to the funded amount column to handle these outliers effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_funded_amount = cap_outliers(fintech_df, 'funded_amount')\n",
    "compare_distributions(fintech_df, 'funded_amount', cap_funded_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'funded_amount', cap_funded_amount)\n",
    "outliers = detect_outliers(fintech_df, 'funded_amount', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the capping method to the `funded_amount` the percentage of outliers is 0%\n",
    "This means that, just like the `loan_amount`, all outliers were successfully handled by the capping method, and there are now no outliers remaining in the `funded_amount` column. This ensures that extreme values won't distort further analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling outliers in the `int_rate`\n",
    "We will apply the log transform on the `int_rate` column as it is positively skewed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_int_rate = get_log_transformation(fintech_df, 'int_rate')\n",
    "compare_distributions(fintech_df, 'int_rate', log_int_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = apply_transformation(fintech_df, 'int_rate', log_int_rate)\n",
    "outliers = detect_outliers(fintech_df, 'int_rate', method='IQR')\n",
    "len(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log transformation for `int_rate` not only shifted the distribution closer to normality but also eliminated all outliers from the `int_rate` column. This significantly reduced the percentage of outliers from 2.294% to 0%, improving the stability of the data for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data transformation and feature eng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Adding Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Month Number Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_month_number(df, date_column):\n",
    "    df[date_column] = pd.to_datetime(df[date_column])\n",
    "    df['month_number'] = df[date_column].dt.month\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = add_month_number(fintech_df, 'issue_date')\n",
    "fintech_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Salary Can Cover Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salary_can_cover(df, log_annual_income_column, loan_amount_column):\n",
    "    # Reverse the log transformation of annual income\n",
    "    df['salary_can_cover'] = (np.exp(df[log_annual_income_column]) >= df[loan_amount_column]).astype(int)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = add_salary_can_cover(fintech_df, 'annual_inc', 'loan_amount')\n",
    "fintech_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Letter Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_grade(grade):\n",
    "        if 1 <= grade <= 5:\n",
    "            return 'A'\n",
    "        elif 6 <= grade <= 10:\n",
    "            return 'B'\n",
    "        elif 11 <= grade <= 15:\n",
    "            return 'C'\n",
    "        elif 16 <= grade <= 20:\n",
    "            return 'D'\n",
    "        elif 21 <= grade <= 25:\n",
    "            return 'E'\n",
    "        elif 26 <= grade <= 30:\n",
    "            return 'F'\n",
    "        elif 31 <= grade <= 35:\n",
    "            return 'G'\n",
    "        else:\n",
    "            return 'Unknown'  # In case there are grades outside the expected range\n",
    "\n",
    "def update_lookup_with_grades(lookup_df):    \n",
    "    for i in range(1, 36):\n",
    "        letter = map_grade(i)\n",
    "        lookup_df = pd.concat([lookup_df, pd.DataFrame([{'column': 'grade', 'original': str(i), 'imputed': letter}])], ignore_index=True)\n",
    "    return lookup_df\n",
    "\n",
    "lookup_df = update_lookup_with_grades(lookup_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_letter_grade(df, grade_column):\n",
    "    df['letter_grade'] = df[grade_column].apply(map_grade)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = add_letter_grade(fintech_df, 'grade')\n",
    "fintech_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Installment per Month Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_installment(df, loan_amount_column, log_int_rate_column, term_column):\n",
    "    df_copy = df.copy()\n",
    "    # Convert term to months (e.g., '36 months' -> 36)\n",
    "    df_copy[term_column] = df_copy[term_column].str.extract('(\\d+)').astype(int)\n",
    "    \n",
    "    # Calculate monthly installment directly in the apply function without adding intermediary columns\n",
    "    df['installment_per_month'] = df_copy.apply(\n",
    "        lambda row: (row[loan_amount_column] * (np.exp(row[log_int_rate_column]) / 12) * (1 + (np.exp(row[log_int_rate_column]) / 12)) ** row[term_column]) / \n",
    "                    ((1 + (np.exp(row[log_int_rate_column]) / 12)) ** row[term_column] - 1)\n",
    "        if np.exp(row[log_int_rate_column]) > 0 else row[loan_amount_column] / row[term_column], axis=1\n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df = calculate_monthly_installment(fintech_df, 'loan_amount', 'int_rate', 'term')\n",
    "fintech_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def label_encode_column(df, column, new_column):\n",
    "    le = LabelEncoder()\n",
    "    df[new_column] = le.fit_transform(df[column])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_columns(df, columns):\n",
    "    for column in columns:\n",
    "        one_hot_encoded = pd.get_dummies(df[column], prefix=column)\n",
    "        # Convert boolean values to integers (0 and 1)\n",
    "        one_hot_encoded = one_hot_encoded.astype(int)\n",
    "        # Concatenate the one-hot encoded columns to the original dataframe\n",
    "        df = pd.concat([df, one_hot_encoded], axis=1)\n",
    "        \n",
    "        df.drop(columns=column, inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded = fintech_df.copy()\n",
    "fintech_df_encoded = label_encode_column(fintech_df_encoded, 'letter_grade','letter_grade_encoded')\n",
    "lookup_df = add_lookup_values(lookup_df, 'letter_grade', fintech_df_encoded['letter_grade'], fintech_df_encoded['letter_grade_encoded'])\n",
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Label Encoding for `letter_grade` because it is an ordinal feature where the order matters. The grades (A-G) follow a sequence where A > B > C, etc., and this relationship needs to be preserved in the encoding.\n",
    "\n",
    "After Label Encoding, the `letter_grade` column now contains integer values representing the grades (e.g., A -> 0, B -> 1, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded = label_encode_column(fintech_df_encoded, 'addr_state', 'addr_state_encoded')\n",
    "fintech_df_encoded = label_encode_column(fintech_df_encoded, 'state','state_encoded')\n",
    "fintech_df_encoded = label_encode_column(fintech_df_encoded, 'purpose','purpose_encoded')\n",
    "\n",
    "lookup_df = add_lookup_values(lookup_df, 'addr_state', fintech_df_encoded['addr_state'], fintech_df_encoded['addr_state_encoded'])\n",
    "lookup_df = add_lookup_values(lookup_df, 'state', fintech_df_encoded['state'], fintech_df_encoded['state_encoded'])\n",
    "lookup_df = add_lookup_values(lookup_df, 'purpose', fintech_df_encoded['purpose'], fintech_df_encoded['purpose_encoded'])\n",
    "\n",
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Label Encoding for `addr_state` and `state` because they have alot of unique values and doing the one-hot-encoding would add alot of rows.\n",
    "\n",
    "After Label Encoding, the `addr_state` and `state` columns now contain integer values representing the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_loan_status(df):\n",
    "    mapping = {'Fully Paid':1,'Current': 2, 'In Grace Period': 3, 'Late (16-30 days)': 4, 'Late (31-120 days)': 5, 'Default': 6, 'Charged Off': 7}\n",
    "    df['loan_status_encoded'] = df['loan_status'].map(mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "fintech_df_encoded = label_encode_loan_status(fintech_df_encoded)\n",
    "lookup_df = add_lookup_values(lookup_df, 'loan_status', fintech_df_encoded['loan_status'], fintech_df_encoded['loan_status_encoded'])\n",
    "\n",
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_verification_status(df):\n",
    "    mapping = {'Not Verified': 1, 'Verified': 2, 'Source Verified': 3}\n",
    "    df['verification_status_encoded'] = df['verification_status'].map(mapping)    \n",
    "    return df\n",
    "\n",
    "fintech_df_encoded = label_encode_verification_status(fintech_df_encoded)\n",
    "lookup_df = add_lookup_values(lookup_df, 'verification_status', fintech_df_encoded['verification_status'], fintech_df_encoded['verification_status_encoded'])\n",
    "\n",
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We used `verification_status`,`loan_status` because it is an ordinal feature where the order matters, and this relationship needs to be preserved in the encoding.\n",
    "\n",
    "After Label Encoding, the `verification_status`,`loan_status` column now contains integer values representing the statuses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded = one_hot_encode_columns(fintech_df_encoded, ['home_ownership', 'term', 'type'])\n",
    "\n",
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used One-Hot Encoding for `home_ownership`, `term`, `type` because these are nominal features where there is no inherent order among the categories. One-hot encoding allows us to represent these categories without introducing any false ordinal relationship.\n",
    "\n",
    "After One-Hot Encoding, the dataset now has binary columns representing each category in `home_ownership`, `term`, `type`. For example, home_ownership_RENT would be 1 if the home ownership status is \"RENT\" and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Normalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already handled the outliers for most of the numerical columns using log transformations, except for the `loan_amount` and `funded_amount` columns. Therefore, we will now proceed to normalize these two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_loan = get_log_transformation(fintech_df_encoded, 'loan_amount')\n",
    "compare_distributions(fintech_df_encoded, 'loan_amount', normalized_loan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed the log transformation did not improve the normalization of the `loan_amount` but this is the best we can do right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_funded = get_log_transformation(fintech_df_encoded, 'funded_amount')\n",
    "compare_distributions(fintech_df_encoded, 'funded_amount', normalized_funded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, s observed the log transformation did not improve the normalization of the `funded_amount` but this is the best we can do right now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Lookup Table(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_df.to_csv('lookup_table.csv', index=False)\n",
    "# Check the lookup table content\n",
    "lookup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Bonus ( Data Integration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_map_state_names(df, state_column):\n",
    "  url = \"https://www23.statcan.gc.ca/imdb/p3VD.pl?Function=getVD&TVD=53971\"\n",
    "  response = requests.get(url)\n",
    "  soup = BeautifulSoup(response.text, 'html.parser')\n",
    "  table = soup.find('table')\n",
    "\n",
    "  # <AlphaCode, StateName>\n",
    "  state_dict = {}\n",
    "\n",
    "  for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    if len(columns) >= 2:  \n",
    "      alpha_code = columns[2].text.strip()\n",
    "      state_name = columns[0].text.strip()  \n",
    "      \n",
    "      state_dict[alpha_code] = state_name\n",
    "\n",
    "  df['state_name'] = df[state_column].map(state_dict)\n",
    "  return df\n",
    "\n",
    "fintech_df_encoded = fetch_and_map_state_names(fintech_df_encoded, 'state')\n",
    "fintech_df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fintech_df_encoded[['state', 'state_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded.drop('state', axis=1, inplace=True)\n",
    "fintech_df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Exporting the dataframe to a csv file or parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded.to_parquet('./fintech_data_MET_P2_52_0812_clean.parquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fintech_df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the correct df saved\n",
    "df = pd.read_parquet('./fintech_data_MET_P2_52_0812_clean.parquet')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
